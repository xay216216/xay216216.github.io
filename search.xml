<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[windows技巧]]></title>
    <url>%2F2021%2F05%2F12%2Fwindows%E6%8A%80%E5%B7%A7%2F</url>
    <content type="text"><![CDATA[一： powershell powershell (new-object Net.WebClient).DownloadFile(‘http://127.0.0.1:8018/11.txt&#39;,&#39;D:\aaaddd\112.txt&#39;)]]></content>
  </entry>
  <entry>
    <title><![CDATA[web请求]]></title>
    <url>%2F2020%2F12%2F19%2Fbaidu%2F</url>
    <content type="text"><![CDATA[输入 http://www.baidu.com 在浏览器的完整过程，越详细越好。浏览器获取输入的域名http://www.baidu.com 浏览器向域名系统DNS请求解析http://www.baidu.com的IP地址 DNS解析出百度服务器的IP地址 浏览器与服务器建立TCP连接（默认端口80） 浏览器发出HTTP请求，请求百度首页 服务器通过HTTP请求把首页文件发给浏览器 TCP连接释放 浏览器解析首页文件，展示web界面 请描述C/C++程序的内存分区? 其实C和C++的内存分区还是有一定区别的，但此处不作区分： 1）、栈区（stack）— 由编译器自动分配释放 ，存放函数的参数值，局部变量的值等。其 操作方式类似于数据结构中的栈。 ///栈先进后出，操作系统编译器自动分配释放 2）、堆区（heap） — 一般由程序员分配释放， 若程序员不释放，程序结束时可能由OS回 收 。注意它与数据结构中的堆是两回事，分配方式倒是类似于链表。 ////我们可以将一条链表想象成环环相扣的结点，就如平常所见到的锁链一样。链表内包含很多结点（当然也可以包含零个结点）。其中每个结点的数据空间一般会包含一个数据结构（用于存放各种类型的数据）以及一个指针，该指针一般称为next，用来指向下一个结点的位置。由于下一个结点也是链表类型，所以next的指针也要定义为链表类型。 3）、全局区（静态区）（static）—，全局变量和静态变量的存储是放在一块的，初始化的 全局变量和静态变量在一块区域， 未初始化的全局变量和未初始化的静态变量在相邻的另 一块区域。 - 程序结束后由系统释放。 4）、文字常量区 —常量字符串就是放在这里的。 程序结束后由系统释放。 5）、程序代码区—存放函数体的二进制代码。 栈区与堆区的区别： 1）堆和栈中的存储内容：栈存局部变量、函数参数等。堆存储使用new、malloc申请的变量等； 2）申请方式：栈内存由系统分配，堆内存由自己申请； 3）申请后系统的响应：栈——只要栈的剩余空间大于所申请空间，系统将为程序提供内存，否则将报异常提示栈溢出。 堆——首先应该知道操作系统有一个记录空闲内存地址的链表，当系统收到程序的申请时，会遍历该链表，寻找第一个空间大于所申请空间的堆结点，然后将该结点从空闲结点链表 中删除，并将该结点的空间分配给程序； 4）申请大小的限制：Windows下栈的大小一般是2M，堆的容量较大； 5）申请效率的比较：栈由系统自动分配，速度较快。堆使用new、malloc等分配，较慢； 总结：栈区优势在处理效率，堆区优势在于灵活； 内存模型：自由区、静态区、动态区； 根据c/c++对象生命周期不同，c/c++的内存模型有三种不同的内存区域，即：自由存储区，动态区、静态区。 自由存储区：局部非静态变量的存储区域，即平常所说的栈； 动态区： 用new ，malloc分配的内存，即平常所说的堆； 静态区：全局变量，静态变量，字符串常量存在的位置； 注：代码虽然占内存，但不属于c/c++内存模型的一部分； 快速排序的思想、时间复杂度、实现以及优化方法? 快速排序的三个步骤 (1)选择基准：在待排序列中，按照某种方式挑出一个元素，作为 “基准”（pivot）； (2)分割操作：以该基准在序列中的实际位置，把序列分成两个子序列。此时，在基准左边的元素都比该基准小，在基准右边的元素都比基准大； (3)递归地对两个序列进行快速排序，直到序列为空或者只有一个元素。 基准的选择： 对于分治算法，当每次划分时，算法若都能分成两个等长的子序列时，那么分治算法效率会达到最大。 即：同一数组，时间复杂度最小的是每次选取的基准都可以将序列分为两个等长的；时间复杂度最大的是每次选择的基准都是当前序列的最大或最小元素； 快排代码实现： 我们一般选择序列的第一个作为基数，那么快排代码如下： void quicksort(vector &amp;v,int left, int right) { if(left &lt; right)//false则递归结束 { int key=v[left];//基数赋值 int low = left; int high = right; while(low &lt; high) //当low=high时，表示一轮分割结束 { while(low &lt; high &amp;&amp; v[high] &gt;= key)//v[low]为基数，从后向前与基数比 较 { high–; } swap(v[low],v[high]); while(low &lt; high &amp;&amp; v[low] &lt;= key)//v[high]为基数，从前向后与基数比 较 { low++; } swap(v[low],v[high]); } //分割后，对每一分段重复上述操作 quicksort(v,left,low-1); quicksort(v,low+1,right); } } 注：上述数组或序列v必须是引用类型的形参，因为后续快排结果需要直接反映在原序列中； 优化： 上述快排的基数是序列的第一个元素，这样的对于有序序列，快排时间复杂度会达到最差的o(n^2)。所以，优化方向就是合理的选择基数。 常见的做法“三数取中”法（序列太短还要结合其他排序法，如插入排序、选择排序等），如下： ①当序列区间长度小于 7 时，采用插入排序； ②当序列区间长度小于 40 时，将区间分成2段，得到左端点、右端点和中点，我们对这三个点取中数作为基数； ③当序列区间大于等于 40 时，将区间分成 8 段，得到左三点、中三点和右三点，分别再得到左三点中的中数、中三点中的中数和右三点中的中数，再将得到的三个中数取中数，然后将该值作为基数。 具体代码只是在上一份的代码中将“基数赋值”改为①②③对应的代码即可： int key=v[left];//基数赋值 if (right-left+1&lt;=7) { insertion_sort(v,left,right);//插入排序 return; }else if(right-left+1&lt;=8){ key=SelectPivotOfThree(v,left,right);//三个取中 }else{ //三组三个取中，再三个取中（使用4次SelectPivotOfThree，此处不具体展示） } 需要调用的函数： void insertion_sort(vector &amp;unsorted,int left, int right) //插入排序算法 { for (int i = left+1; i &lt;= right; i++) { if (unsorted[i - 1] &gt; unsorted[i]) { int temp = unsorted[i]; int j = i; while (j &gt; left &amp;&amp; unsorted[j - 1] &gt; temp) { unsorted[j] = unsorted[j - 1]; j–; } unsorted[j] = temp; } } } int SelectPivotOfThree(vector &amp;arr,int low,int high) //三数取中，同时将中值移到序列第一位 { int mid = low + (high - low)/2;//计算数组中间的元素的下标 //使用三数取中法选择枢轴 if (arr[mid] &gt; arr[high])//目标: arr[mid] &lt;= arr[high] { swap(arr[mid],arr[high]); } if (arr[low] &gt; arr[high])//目标: arr[low] &lt;= arr[high] { swap(arr[low],arr[high]); } if (arr[mid] &gt; arr[low]) //目标: arr[low] &gt;= arr[mid] { swap(arr[mid],arr[low]); } //此时，arr[mid] &lt;= arr[low] &lt;= arr[high] return arr[low]; //low的位置上保存这三个位置中间的值 //分割时可以直接使用low位置的元素作为枢轴，而不用改变分割函数了 } 这里需要注意的有两点： ①插入排序算法实现代码； ②三数取中函数不仅仅要实现取中，还要将中值移到最低位，从而保证原分割函数依然可用。 请描述IO多路复用机制? IO模型有4中：同步阻塞IO、同步非阻塞IO、异步阻塞IO、异步非阻塞IO；IO多路复用属于IO模型中的异步阻塞IO模型，在服务器高性能IO构建中常常用到。 同步异步是表示服务端的，阻塞非阻塞是表示用户端，所以可解释为什么IO多路复用（异步阻塞）常用于服务器端的原因； 文件描述符（FD，又叫文件句柄）：描述符就是一个数字，它指向内核中的一个结构体(文件路径，数据区等属性)。具体来源：Linux内核将所有外部设备都看作一个文件来操作，对文件的操作都会调用内核提供的系统命令，返回一个fd(文件描述符)。 下面开始介绍IO多路复用： （1）I/O多路复用技术通过把多个I/O的阻塞复用到同一个select、poll或epoll的阻塞上，从而使得系统在单线程的情况下可以同时处理多个客户端请求。与传统的多线程/多进程模型比，I/O多路复用的最大优势是系统开销小，系统不需要创建新的额外进程或者线程。 （2）select，poll，epoll本质上都是同步I/O，因为他们都需要在读写事件就绪后自己负责进行读写，也就是说这个读写过程是阻塞的，而异步I/O则无需自己负责进行读写，异步I/O的实现会负责把数据从内核拷贝到用户空间。 （3）I/O多路复用的主要应用场景如下： 服务器需要同时处理多个处于监听状态或者多个连接状态的套接字； 服务器需要同时处理多种网络协议的套接字； （4）目前支持I/O多路复用的系统调用有 select，poll，epoll，epoll与select的原理比较类似，但epoll作了很多重大改进，现总结如下： ①支持一个进程打开的文件句柄FD个数不受限制（为什么select的句柄数量受限制：select使用位域的方式来传递关心的文件描述符，因为位域就有最大长度，在Linux下是1024，所以有数量限制）； ②I/O效率不会随着FD数目的增加而线性下降； ③epoll的API更加简单； （5）三种接口调用介绍： ①select函数调用格式： #include &lt;sys/select.h&gt; #include &lt;sys/time.h&gt; int select(int maxfdp1,fd_set readset,fd_set writeset,fd_set exceptset,const struct timeval timeout) //返回值：就绪描述符的数目，超时返回0，出错返回-1 ②poll函数调用格式： include &lt;poll.h&gt;int poll ( struct pollfd * fds, unsigned int nfds, int timeout); ③epoll函数格式（操作过程包括三个函数）： #include &lt;sys/epoll.h&gt; int epoll_create(int size); int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event); int epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout); （6）作用：一定程度上替代多线程/多进程，减少资源占用，保证系统运行的高效率； 使用mysql索引都有哪些原则？索引什么数据结构？B+tree 和 B tree 什么区别？ 1、 对于查询频率高的字段创建索引； 2、 对排序、分组、联合查询频率高的字段创建索引； 3、 索引的数目不宜太多 原因： a、每创建一个索引都会占用相应的物理控件； b、过多的索引会导致insert、update、delete语句的执行效率降低； 4、若在实际中，需要将多个列设置索引时，可以采用多列索引 如：某个表(假设表名为Student)，存在多个字段(StudentNo, StudentName, Sex, Address, Phone,BirthDate)，其中需要对StudentNo,StudentName字段进行查询，对Sex字段进行分组，对BirthDate 字段进行排序，此时可以创建多列索引index index_name (StudentNo, StudentName, Sex, BirthDate);#index_name为索引名 在上面的语句中只创建了一个索引，但是对4个字段都赋予了索引的功能。 创建多列索引，需要遵循BTree类型，即第一列使用时，才启用索引。 在上面的创建语句中，只有mysql语句在使用到StudentNo字段时，索引才会被启用。 如: select * from Student where StudentNo = 1000; #使用到了StudentNo字段，索引被启用。 以使用explain检测索引是否被启用如:explain select * from Student where StudentNo = 1000; 5、选择唯一性索引 唯一性索引的值是唯一的，可以更快速的通过该索引来确定某条记录。例如，学生表中学号是具有唯 一性的字段。为该字段建立唯一性索引可以很快的确定某个学生的信息。如果使用姓名的话，可能存 在同名现象，从而降低查询速度。 6、尽量使用数据量少的索引 如果索引的值很长，那么查询的速度会受到影响。例如，对一个CHAR(100)类型的字段进行全文检索 需要的时间肯定要比对CHAR(10)类型的字段需要的时间要多。 7、尽量使用前缀来索引 如果索引字段的值很长，最好使用值的前缀来索引。例如，TEXT和BLOG类型的字段，进行全文检 索会很浪费时间。如果只检索字段的前面的若干个字符，这样可以提高检索速度。 8、删除不再使用或者很少使用的索引. 表中的数据被大量更新，或者数据的使用方式被改变后，原有的一些索引可能不再需要。数据库管理 员应当定期找出这些索引，将它们删除，从而减少索引对更新操作的影响B+ tree树索引, B tree,散列 Mysql有哪些存储引擎？请详细列举其区别？ InnoDB: 事务型存储引擎, 并且有较高的并发读取频率 MEMORY： 存储引擎，存放在内存中，数据量小， 速度快 Merge: ARCHIVE： 归档， 有很好的压缩机制 设计高并发系统数据库层面该如何设计？ 数据库锁有哪些类型？如何实现？ 分库分表： 同样量的数据平均存储在不同数据库相同表（或不同表）中，减轻单表压力，如果还是很大，就可以每个库在分多张表，根据hash取值或者其他逻辑判断将数据存储在哪张表中 读写分离： 数据库原本就有主从数据库之分，查询在从服务器，增删改在主服务器， 归档和操作表区分： 建一张归档表，将历史数据放入，需要操作的表数据单独存储 索引啊之类的创建，对于数据量很大，百万级别以上的单表，如果增删改操作不频繁的话， 可以创建bitMap索引，速度要快得多 共享锁：要等第一个人操作完，释放锁，才能操作 更新锁：解决死锁，别人可以读，但不能操作 排他锁：读写都被禁用 意向锁（xlock）： 对表中部分数据加锁，查询时，可以跳过 计划锁： 操作时，别的表连接不了这张表， 常见内存分配算法及优缺点如下： （1）首次适应算法。使用该算法进行内存分配时，从空闲分区链首开始查找，直至找到一个能满足其大小需求的空闲分区为止。然后再按照作业的大小，从该分区中划出一块内存分配给请求者，余下的空闲分区仍留在空闲分区链中。 该算法倾向于使用内存中低地址部分的空闲分区，在高地址部分的空闲分区非常少被利用，从而保留了高地址部分的大空闲区。显然为以后到达的大作业分配大的内存空间创造了条件。缺点在于低址部分不断被划分，留下许多难以利用、非常小的空闲区，而每次查找又都从低址部分开始，这无疑会增加查找的开销。 （2）循环首次适应算法。该算法是由首次适应算法演变而成的。在为进程分配内存空间时，不再每次从链首开始查找，而是从上次找到的空闲分区开始查找，直至找到一个能满足需求的空闲分区，并从中划出一块来分给作业。该算法能使空闲中的内存分区分布得更加均匀，但将会缺乏大的空闲分区。 （3）最佳适应算法。该算法总是把既能满足需求，又是最小的空闲分区分配给作业。 为了加速查找，该算法需求将所有的空闲区按其大小排序后，以递增顺序形成一个空白链。这样每次找到的第一个满足需求的空闲区，必然是最优的。孤立地看，该算法似乎是最优的，但事实上并不一定。因为每次分配后剩余的空间一定是最小的，在存储器中将留下许多难以利用的小空闲区。同时每次分配后必须重新排序，这也带来了一定的开销。 （4）最差适应算法。最差适应算法中，该算法按大小递减的顺序形成空闲区链，分配时直接从空闲区链的第一个空闲分区中分配（不能满足需要则不分配）。非常显然，如果第一个空闲分区不能满足，那么再没有空闲分区能满足需要。这种分配方法初看起来不太合理，但他也有非常强的直观吸引力：在大空闲区中放入程式后，剩下的空闲区常常也非常大，于是还能装下一个较大的新程式。 最坏适应算法和最佳适应算法的排序正好相反，他的队列指针总是指向最大的空闲区，在进行分配时，总是从最大的空闲区开始查寻。 该算法克服了最佳适应算法留下的许多小的碎片的不足，但保留大的空闲区的可能性减小了，而且空闲区回收也和最佳适应算法相同复杂。]]></content>
      <categories>
        <category>go</category>
      </categories>
      <tags>
        <tag>go</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[go 高并发]]></title>
    <url>%2F2020%2F12%2F19%2Fgo%E9%AB%98%E5%B9%B6%E5%8F%91%2F</url>
    <content type="text"><![CDATA[一 aaa cat -n b.txt &gt; d.txt 将b.txt的内容写到d.txt，并携带行数]]></content>
      <categories>
        <category>go</category>
      </categories>
      <tags>
        <tag>go</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hook语法]]></title>
    <url>%2F2020%2F11%2F19%2Fhook%2Cdefer%2F</url>
    <content type="text"><![CDATA[一、什么是 Hook 技术 Hook 技术又叫做钩子函数，在系统没有调用该函数之前，钩子程序就先捕获该消息，钩子函数先得到控制权，这时钩子函数既可以加工处理（改变）该函数的执行行为，还可以强制结束消息的传递。简单来说，就是把系统的程序拉出来变成我们自己执行代码片段。 要实现钩子函数，有两个步骤： 1. 利用系统内部提供的接口，通过实现该接口，然后注入进系统（特定场景下使用） 2.动态代理（使用所有场景） 二、Hook 技术实现的步骤 Hook 技术实现的步骤也分为两步 1.找到 hook 点（Java 层），该 hook 点必须满足以下的条件：需要 hook 的方法，所属的对象必须是静态的，因为我们是通过反射来获取对象的，我们获取的是系统的对象，所以不能够 new 一个新的对象，必须用系统创建的那个对象，所以只有静态的才能保证和系统的对象一致。 2.将 hook 方法放到系统之外执行（放入我们自己的逻辑） 三、使用 hook 技术实现免注册式跳转 解释一下上面的步骤，我们有一个 MainActivity，四个按钮，前三个是打开不同的 Activity，最后一个是退出登录，这三个 Activity 其中界面2是不需要登录的，界面3和界面4都是需要登录才能看到的。 那么既然要在打开 Activity 之前就判断是否登录了，而且要使用 hook 技术，那么我们下看一下 startActivity 的源码，因为我们知道我们需要 hook 的就是 startActivity 方法。 找 Hook 点 解释一下上面的源码，ActivityManager.getService()方法调用的是 IActivityManagerSingleton.get()方法，而这个IActivityManagerSingleton是 Singleton(android.util)，所以 IActivityManagerSingleton.get()就是调用了 Singleton 里面的 get 方法，进到 Singleton 类，发现 get() 方法里面会通过 create() 抽象方法方法给 mInstance 属性赋值，回到刚才的地方，我们发现，create() 方法返回了一个 IAcivityManager 对象。最终结果：其实最终是 IActivityManager 调用了 startActivity() 方法。 所以我们真正想 hook 的点是 IActivityManager 对象，那么如何拿到这个静态对象呢？其实聪明的帅哥和美女肯定都发现了，这个 IActivityManagerSingleton 其实就是一个静态的，而且我们拿到该系统对象后就获取到该对象的 mInstance 属性，即 IActivityManager，那么我们就把 IActivityManagerSingleton 当做一个伪 hook 点。 hook 点已经找到了，第一步已经完成，接下来就该第二步了，那么如何将系统执行的 startActivity() 拉到系统外执行，给其添加一些自己的逻辑呢？这里我们使用动态代理来实现。 这里大概说一下项目，肯定有五个 Activity，一个 MainActivity 是用来展示四个按钮的，一个 LoginActivity，还有其他三个是测试的展示页面，其实还有一个 ProxyActivity，并且，在清单文件中，我们除了 MainActivity 是启动页，ProxyActivity 进行了注册，其他的 Activity 都没有在清单文件中注册，没错，你没有看错，就是没有注册，那运行会崩溃吗？空口无凭，我们先看一下代码，然后看运行结果。 四 移动存储代码示例 func init() { // Register Service server.RegisterService( // Service NewService(), 第一执行 // StartHook frameworkServer.WithServiceStartHook(func(provider frameworkServer.ServiceProvider) error { // CheckErrors err := common.CheckErrors( 第四执行 initProvider(provider), initOrmAndRegisterTables(), initStorage(), ) if err != nil { return err } // run after init run() 第五执行 return nil }), ) initFlags() 第二执行 registerModels() *** 第三执行} 五 defer 表示对紧跟其后的 xxx() 函数延迟到 defer 语句所在的当前函数返回时()再进行调用。 移动存储示例代码 func main() { defer da.Init()() *** 双括号表示 会第一执行da.Init()。然后把da.Init()看作一个函数T，T()最后第五执行 crossApi.RegisterAllService() *** 第二执行 if err := da.RunServer(); err != nil { *** 第四执行 panic(&quot;run service error:&quot; + err.Error()) } } package main import &quot;fmt&quot; var g = 100 package main import &quot;fmt&quot; var g = 100 func f() (r int) { r = g defer func() { r = 200 }() fmt.Printf(&quot;f: r = %d\n&quot;, r) r = 0 return r } func main() { i := f() fmt.Printf(&quot;main: i = %d, g = %d\n&quot;, i, g) } f: r =100 main: i =200, g =100 准确的说，defer 函数的执行既不是在 return 之后也不是在 return 之前，而是一条go语言的 return 语句包含了对 defer 函数的调用，即 return 会被翻译成如下几条伪指令。go 语言的 return 会被编译器翻译成多条指令，其中包括保存返回值，调用defer注册的函数以及实现函数返回。]]></content>
      <tags>
        <tag>grpc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[etcd]]></title>
    <url>%2F2020%2F09%2F08%2Fetcd%2F</url>
    <content type="text"><![CDATA[etcd定义 要问etcd是什么？很多人第一反应可能是一个键值存储仓库，却没有重视官方定义的后半句，用于配置共享和服务发现。 是一个分布式的，一致的 key-value 存储，主要用途是共享配置和服务发现。Etcd 已经在很多分布式系统中得到广泛的使用，本文的架构与实现部分主要解答以下问题： Etcd是如何实现一致性的？ Etcd的存储是如何实现的？ Etcd的watch机制是如何实现的？ Etcd的key过期机制是如何实现的？ 提供存储以及获取数据的接口，它通过协议保证 Etcd 集群中的多个节点数据的强一致性。用于存储元信息以及共享配置。 提供监听机制，客户端可以监听某个key或者某些key的变更（v2和v3的机制不同，参看后面文章）。用于监听和推送变更。 提供key的过期以及续约机制，客户端通过定时刷新来实现续约（v2和v3的实现机制也不一样）。用于集群监控以及服务注册发现。 提供原子的CAS（Compare-and-Swap）和 CAD（Compare-and-Delete）支持（v2通过接口参数实现，v3通过批量事务实现）。用于分布式锁以及leader选举。 raft通过对不同的场景（选主，日志复制）设计不同的机制，虽然降低了通用性（相对paxos），但同时也降低了复杂度，便于理解和实现。raft内置的选主协议是给自己用的，用于选出主节点，理解raft的选主机制的关键在于理解raft的时钟周期以及超时机制。 理解 Etcd 的数据同步的关键在于理解raft的日志同步机制。 Etcd 实现raft的时候，充分利用了go语言CSP并发模型和chan的魔法，想更进行一步了解的可以去看源码，这里只简单分析下它的wal日志。 wal日志是二进制的，解析出来后是以上数据结构LogEntry。其中第一个字段type，只有两种，一种是0表示Normal，1表示ConfChange（ConfChange表示 Etcd 本身的配置变更同步，比如有新的节点加入等）。第二个字段是term，每个term代表一个主节点的任期，每次主节点变更term就会变化。第三个字段是index，这个序号是严格有序递增的，代表变更序号。第四个字段是二进制的data，将raft request对象的pb结构整个保存下。Etcd 源码下有个tools/etcd-dump-logs，可以将wal日志dump成文本查看，可以协助分析raft协议。 raft算法 Raft为了避免这种情况发生，而规定了一个原则，Raft 永远不会通过计算副本数目的方式去提交一个之前任期内的日志条目， 相关链接地址： https://blog.csdn.net/bbwangj/article/details/82584988 https://www.jianshu.com/p/b93e883b92ea https://www.cnblogs.com/mokafamily/p/11303534.html https://github.com/maemual/raft-zh_cn/blob/master/raft-zh_cn.md]]></content>
      <categories>
        <category>etcd</category>
      </categories>
      <tags>
        <tag>etcd</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis笔记]]></title>
    <url>%2F2020%2F06%2F18%2Fredis%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[cmd访问redisredis-cli -h 127.0.0.1 -p 6379 -a XXX 1.基于内存的key-value数据库2.基于c语言编写的，可以支持多种语言的api //set每秒11万次，取get 81000次3.支持数据持久化4.value可以是string，hash， list， set, sorted set 使用场景 去最新n个数据的操作 排行榜，取top n个数据 //最佳人气前10条 精确的设置过期时间 计数器 实时系统， 反垃圾系统 pub， sub发布订阅构建实时消息系统 构建消息队列 缓存 模糊查询 第一种： 第二种：？ 第三种：[] // 查询所有的key keys // key中含有keyword 的key keys *keyword* // 你知道前面的一些字母，忘记了最后一个字母 keys hell？ // 你只记得第一个字母是h，他的长度是5 keys h？？？？ // 你知道前面四个字母，最后一个字母有可能是o p t 其中的一个 keys hell[opt] key keys 获取所有的key keys tes //含有tes的key select 0 选择第一个库 move myString 1 将当前的数据库key移动到某个数据库,目标库有，则不能移动 flush db 清除指定库 randomkey 随机key type key 类型 set key1 value1 设置key get key1 获取key mset key1 value1 key2 value2 key3 value3 mget key1 key2 key3 del key1 删除key exists key 判断是否存在key expire key 10 10过期 pexpire key 1000 毫秒 persist key 删除过期时间 string set name cxx get name getrange name 0 -1 字符串分段 getset name new_cxx 设置值，返回旧值 mset key1 key2 批量设置 mget key1 key2 批量获取 setnx key value 不存在就插入（not exists） setex key time value 过期时间（expire） setrange key index value 从index开始替换value incr age 递增 incrby age 10 递增 decr age 递减 decrby age 10 递减 incrbyfloat 增减浮点数 append 追加 strlen 长度 getbit/setbit/bitcount/bitop 位操作 hash hset myhash name cxx hget myhash name hmset myhash name cxx age 25 note “i am notes” hmget myhash name age note hgetall myhash 获取所有的 hexists myhash name 是否存在 hsetnx myhash score 100 设置不存在的 hincrby myhash id 1 递增 hdel myhash name 删除 hkeys myhash 只取key hvals myhash 只取value hlen myhash 长度 list lpush mylist a b c 左插入 rpush mylist x y z 右插入 lrange mylist 0 -1 数据集合 lpop mylist 弹出元素 rpop mylist 弹出元素 llen mylist 长度 lrem mylist count value 删除 lindex mylist 2 指定索引的值 lset mylist 2 n 索引设值 ltrim mylist 0 4 删除key linsert mylist before a 插入 linsert mylist after a 插入 rpoplpush list list2 转移列表的数据 set sadd myset redis smembers myset 数据集合 srem myset set1 删除 sismember myset set1 判断元素是否在集合中 scard key_name 个数 sdiff | sinter | sunion 操作：集合间运算：差集 | 交集 | 并集 srandmember 随机获取集合中的元素 spop 从集合中弹出一个元素 zset zadd zset 1 one zadd zset 2 two zadd zset 3 three zincrby zset 1 one 增长分数 zscore zset two 获取分数 zrange zset 0 -1 withscores 范围值 zrangebyscore zset 10 25 withscores 指定范围的值 zrangebyscore zset 10 25 withscores limit 1 2 分页 Zrevrangebyscore zset 10 25 withscores 指定范围的值 zcard zset 元素数量 Zcount zset 获得指定分数范围内的元素个数 Zrem zset one two 删除一个或多个元素 Zremrangebyrank zset 0 1 按照排名范围删除元素 Zremrangebyscore zset 0 1 按照分数范围删除元素 Zrank zset 0 -1 分数最小的元素排名为0 Zrevrank zset 0 -1 分数最大的元素排名为0 Zinterstore zunionstore rank:last_week 7 rank:20150323 rank:20150324 rank:20150325 weights 1 1 1 1 1 1 1 排序： sort mylist 排序 sort mylist alpha desc limit 0 2 字母排序 sort list by it: desc by命令 sort list by it: desc get it: get参数 sort list by it: desc get it:* store sorc:result sort命令之store参数：表示把sort查询的结果集保存起来 订阅与发布： 订阅频道：subscribe chat1 发布消息：publish chat1 “hell0 ni hao” 查看频道：pubsub channels 查看某个频道的订阅者数量: pubsub numsub chat1 退订指定频道： unsubscrible chat1 , punsubscribe java. 订阅一组频道： psubscribe java. redis事物： 隔离性，原子性， 步骤： 开始事务，执行命令，提交事务 multi //开启事务 sadd myset a b c sadd myset e f g lpush mylist aa bb cc lpush mylist dd ff gg 服务器管理 dump.rdb appendonly.aof //BgRewriteAof 异步执行一个aop(appendOnly file)文件重写 会创建当前一个AOF文件体积的优化版本 //BgSave 后台异步保存数据到磁盘，会在当前目录下创建文件dump.rdb //save同步保存数据到磁盘，会阻塞主进程，别的客户端无法连接 //client kill 关闭客户端连接 //client list 列出所有的客户端 //给客户端设置一个名称 client setname myclient1 client getname config get port //configRewrite 对redis的配置文件进行改写 rdb save 900 1save 300 10save 60 10000 aop备份处理appendonly yes 开启持久化appendfsync everysec 每秒备份一次 命令：bgsave异步保存数据到磁盘（快照保存）lastsave返回上次成功保存到磁盘的unix的时间戳shutdown同步保存到服务器并关闭redis服务器bgrewriteaof文件压缩处理（命令） 链接地址 https://www.cnblogs.com/cxxjohnson/p/9072383.html]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[grpc语法]]></title>
    <url>%2F2020%2F05%2F19%2Fgrpc%E8%AF%AD%E6%B3%95%2F</url>
    <content type="text"><![CDATA[一：语法 首先头部的syntax=”proto3”是必须的，否则他将按proto2编译。其次因为grpc无论是否有数据要发送返回，你都必须定义发送数据和返回数据。所以我们在这里定义了一个空的数据以方便我们在不需要传数据时使用。 protobuf在使用中只有基本类型，如果数据时一个list，你则需要用到repeatred这个关键字，他代表你的这个数据是不定的，可以是0个，1个或者更多。 oneof ：可以让你包裹的几个字段只能有一个被赋值。 在proto2中的关键字，在proto3中已经删除： required ： 表示此字段必填。此字段需要小心设置因为如果忘记设置带有此关键字的字段，你的数据将被拒绝解析。 optional : 可选字段。（在proto3中默认就是可选的）]]></content>
      <tags>
        <tag>grpc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vim笔记]]></title>
    <url>%2F2020%2F05%2F18%2Fvim%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[一： vim命令 https://www.cnblogs.com/yangjig/p/6014198.html https://blog.csdn.net/meiceatcsdn/article/details/80562183 1.字符 x 删除当前光标下的字符 X 删除当前光标左边的字符 s 修改一个字符 diw 删除光标所在的单词，不包括空白字符 dw Esc –退出编辑模式，进入命令模式 x –命令模式下，相当于我们平时用的删除键。 Delete –命令模式下，跟我们平时用的del键是一样的 dd –删除该光标所在的行，将整行都给删除掉。 u 撤消最后一次修改 U 撤消当前行的所有修改 y0 复制光标所在行中的首字母到光标所在的字母之间的数据 (不包含光标上的字母） y$ 复制光标所在行中的光标所在字母到行尾之间的数据（包含光标上的字母） y1G 复制第1行到光标所在行的所有数据 yG 复制光标所在行到最后一行所有的数据 0 (零按键) 移动到当前行的行首。 $ 移动到当前行的末尾。 G 移动到文件末尾。 H 将光标移动到屏幕的顶行 nH 将光标移动到屏幕顶行下的第 n 行 n 在同一方向重复查找 N 在相反方向重复查找]]></content>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[gin--用户管理系统]]></title>
    <url>%2F2020%2F02%2F23%2Fgin-%E7%94%A8%E6%88%B7%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"><![CDATA[一 需求分析 功能点列表 管理员登录 登录 退出 gin--用户管理系统 用户查询 增加 删除 修改 二 E_R设计及接口准备 CREATE TABLE user (id BIGINT(20) UNSIGNED NOT NULL AUTO_INCREMENT COMMENT ‘自增id’,name VARCHAR(255) NOT NULL DEFAULT ‘’ COMMENT ‘姓名’,addr VARCHAR(255) NOT NULL DEFAULT ‘’ COMMENT ‘地址’,age SMALLINT(4) NOT NULL DEFAULT 0 COMMENT ‘年龄’,birth VARCHAR(100) NOT NULL DEFAULT ‘2000-01-01 00:00:00’ COMMENT ‘生日’,sex SMALLINT(4) NOT NULL DEFAULT 0 COMMENT ‘性别’,update_at datetime NOT NULL DEFAULT ‘1970-01-01 00:00:00’ COMMENT ‘更新时间’,create_at datetime NOT NULL DEFAULT ‘1970-01-01 00:00:00’ COMMENT ‘创建时间’,PRIMARY KEY (id))ENGINE=INNODB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8 COMMENT=’用户表’; 接口文档base_url http://127.0.0.1:8880 管理登录接口地址：/login类型 post参数 username string 必填 password string 必填返回{ “errno”: 0, “errmsg”: “”, “data”: { “username”: “admin”, “password”: “123456” }, “trace_id”: “c0a8fd015e5281a8cf88297cf27cc6b0”} 管理登录接口地址：/loginout类型 get参数返回 {} 用户列表地址：/user/listpage类型 post参数 page string 必填 name string 选填返回 {} 用户列表 http://127.0.0.1:8880/api/user/add?name=xiaoayong9&amp;age=28&amp;birth=2019-01-01&amp;addr=湖北省荆州市石首市&amp;sex=1地址：/user/add类型 post get参数 birth string 必填 name string 必填 sex int 必填 age int 必填 addr string 必填返回{ “errno”: 0, “errmsg”: “”, “data”: “”, “trace_id”: “c0a8fd015e52831aa040297cbb5389b0”} 用户列表 http://127.0.0.1:8880/api/user/listpage?page=1地址：/user/edit类型 post get参数 id int 必填 birth string 必填 name string 必填 sex int 必填 age int 必填 addr string 必填返回 {} 用户列表 http://127.0.0.1:8880/api/user/remove?ids=12,13,14,15地址：/user/remove, /user/batchremove类型 post get参数 ids返回{ “errno”: 0, “errmsg”: “”, “data”: “”, “trace_id”: “c0a8fd015e5283d2b39c297c6b36dbb0”}]]></content>
      <categories>
        <category>goland</category>
      </categories>
      <tags>
        <tag>golang gin</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[gin学习笔记]]></title>
    <url>%2F2020%2F02%2F20%2Fgin%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[win7 定时关机 在cmd窗口 输入shutdown -f -s -t 3600 回车(一个小时后关机)，然后在桌面的右下角就会弹出多久关闭计算机的提示了 win7 切换目录 cd /d d:/redis win7 启动本地redis redis-server.exe redis.windows.conf 另外起一个cmd redis-cli.exe -h 127.0.0.1 -p 6379 -a 123456 //123456是密码一：安装部署 1. mkdir -p xay216216/gin_test_project cd $_ export GO111MODULE=on 开启go mod go mod init 初始化 go get -v github.com/gin-gonic/gin@v1.4 （这步很可能下不下来 go env GO111MODULE=on go env GOPROXY=https://mirrors.aliyun.com/goproxy/,direct） 还可以手动下载 go get github.com/gin-gonic/gin@v1.4 Go 1.11 版本开始，官方支持了 go module 包依赖管理工具。 查看系统环境变量 windows系统的电脑自己进去添加 export GO111MODULE=on export GOPROXY=https://goproxy.io 但是很多人都是喜欢用Goland来进行处理下载依赖包，上述设置完毕在Goland中还是无法进行下载依赖的，还需要如下设置： 在goland-&gt;setting-&gt;GO-&gt;GO MODULE-&gt;Proxy 填入 https://goproxy.io 新建项目时要勾选Enable Go Modules(vgo) 二：测试 curl -X GET “http://127.0.0.1:8080/ping&quot;三：请求路由 1 多种请求类型 r.GET(“/get”, func(c gin.Context) { c.JSON(200, gin.H{ “message”:”pong”, }) r.POST(“/post”, func(c gin.Context) { c.String(200,”success post”) }) r.Handle(“DELETE”, “/delete”, func(c gin.Context) { c.String(200,”success_delete”) }) r.Any(“/any”, func(c gin.Context) { // GET, POST, PUT, PATCH, HEAD, OPTIONS, DELETE, CONNECT, TRACE 9请求种方式 c.String(200, “success_any”) }) 2 静态文件夹 r.Static(“/assets”, “./assets”) r.StaticFS(“/static”, http.Dir(“sttatic”)) r.StaticFile(“/favicon.ico”, “./favicon.ico”) windows查看8080端口 netstat -ano | findstr 8080 tskill 10796 杀掉对应的进程 3 参数 r.GET(&quot;/:name/:id&quot;, func(c *gin.Context) { c.JSON(200, gin.H{ &quot;name&quot;: c.Param(&quot;name&quot;), &quot;id&quot;: c.Param(&quot;id&quot;), }) }) curl -X GET &quot;http://127.0.0.1:8080/xay/99&quot; firstName:=c.Query(&quot;first_name&quot;) lastName:=c.DefaultQuery(&quot;last_name&quot;,&quot;xiaoa&quot;) c.String(http.StatusOK,&quot;%s,%s&quot;, firstName, lastName) curl -X GET &quot;http://127.0.0.1:8080/test?first_name=sss&amp;last_name=ddd&quot; post bodyByts, err := ioutil.ReadAll(c.Request.Body) if err != nil { c.String(http.StatusBadRequest, err.Error()) c.Abort() } c.Request.Body = ioutil.NopCloser(bytes.NewBuffer(bodyByts)) firstName := c.PostForm(&quot;first_name&quot;) lastName := c.DefaultPostForm(&quot;last_name&quot;, &quot;xiaoayong&quot;) c.String(http.StatusOK, &quot;%s,%s,%s&quot;, firstName, lastName, string(bodyByts)) curl -X POST &quot;http://127.0.0.1:8080/test&quot; -d &apos;{&quot;{&quot;name&quot;:&quot;xay&quot;}&apos; curl -X POST &quot;http://127.0.0.1:8080/test&quot; -d &apos;first_name=sss&amp;last_name=ddd&apos; struct var person Person //这里根据请求content-type来做不同的binding操作 if err:= c.ShouldBind(&amp;person); err==nil { c.String(http.StatusOK,&quot;%v&quot;, person) }else{ c.String(http.StatusOK,&quot;param bind err:%v&quot;, err) } curl -X GET &quot;http://127.0.0.1:8080/testing?name=xiaoayong&amp;address=jingzhu&amp;birthday=2019-09-09&quot; curl -X POST &quot;http://127.0.0.1:8080/testing?name=xiaoayong&amp;address=jingzhu&amp;birthday=2019-09-09&quot; curl -X POST &quot;http://127.0.0.1:8080/testing&quot; -d &apos;name=xiaoayong&amp;address=jingzhu&amp;birthday=2019-09-09&apos; curl -H &quot;content-type:application/json&quot; -X POST &quot;http://127.0.0.1:8080/testing&quot; -d &apos;{&quot;name&quot;:&quot;xiao&quot;,&quot;address&quot;:&quot;wuhan&quot;}&apos; json格式 4 验证请求参数 type Person struct { Age int `form:&quot;age&quot; validate:&quot;required,gt=10&quot;` Name string `form:&quot;name&quot; validate:&quot;required&quot;` Address string `form:&quot;address&quot; validate:&quot;required&quot;` 二 中间件 服务器优雅关停 1.传统 开始-&gt;gin实例-&gt;gin run阻塞-&gt;结束 2.优雅 开始-&gt;gin实例-&gt;server listen and server -&gt;结束 os.signal （设定超时5秒） 何为优雅，就是关闭服务器端口服务后，关闭后的请求不处理，关闭前的所有请求要正常处理 三： 模板 四： go mod 安装软件依赖 go mod使用请查阅： https://blog.csdn.net/e421083458/article/details/89762113 export GO111MODULE=on export GOPROXY=https://goproxy.io case 1) 直接从github上面clone一个项目下来。直接执行 go mod init 会自动生成带git地址的packagename case 2) 不带git的项目 go mod init packagename 下载依赖包 go mod download 拉取必须模块，移除不用的模块 go mod tidy 将依赖包下载到vendor目录 go mod vendor]]></content>
      <categories>
        <category>goland</category>
      </categories>
      <tags>
        <tag>golang</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[goland快捷键]]></title>
    <url>%2F2020%2F02%2F19%2Fgoland%E5%BF%AB%E6%8D%B7%E9%94%AE%2F</url>
    <content type="text"><![CDATA[GoLand常用快捷键 CTRL+ALT ←/→ 返回上次编辑的位置 CTRL+N 查找类 CTRL+SHIFT+N 查找文件 CTRL+SHIFT+ALT+N 查找类中的方法或变量 SHIFT+SHIFT 全局查找 CTRL+E 最近打开的文件 Alt+Enter 导入包,自动修正 红线、黄线都可以用 ALT+SHIFT up/down 上下移动一行代码 CTRL+F 在当前窗口查找文本 CTRL+SHIFT+F 在指定窗口查找文本 CTRL+SHIFT+C 复制文件所在路径 CTRL+ALT+L 格式化代码 ALT+INSERT 生成代码(如GET,SET方法,构造函数等) CTRL+SHIFT+F10 RUN CTRL+ALT+SHIFT+F go fmt file CTRL+ALT+SHIFT+P go fmt project 1、查询快捷键 CTRL+B 快速打开光标处的类或方法 CTRL+ALT+B 找所有的子类 CTRL+SHIFT+B 找变量的类 CTRL+G 定位行 CTRL+R 在 当前窗口替换文本 CTRL+SHIFT+R 在指定窗口替换文本 ALT+SHIFT+C 查找修改的文件 F3 向下查找关键字出现位置 SHIFT+F3 向上一个关键字出现位置 F4 查找变量来源 CTRL+ALT+F7 选中的字符查找工程出现的地方 CTRL+SHIFT+O 弹出显示查找内容 ALT+F1 查找文件所在目录位置 CTRL+H 显示类结构图 CTRL+Q 显示注释文档 CTRL+SHIFT+I 简短查看源定义 CTRL+SHIFT+P 查看表达式类型（在表达式上或者表达式末尾使用）2、自动代码 CTRL+ALT+I 自动缩进 CTRL+ALT+O 优化导入的类和包 ALT+INSERT 生成代码(如GET,SET方法,构造函数等) CTRL+SHIFT+SPACE 自动补全代码 CTRL+空格 代码提示 CTRL+ALT+SPACE 类名或接口名提示 CTRL+P 方法参数提示 CTRL+J 自动代码 CTRL+ALT+T 把选中的代码放在 TRY{} IF{} ELSE{} 里 SHIFT+F6 重构-重命名 CTRL+I 实现接口 CTRL+SHIFT+Space 智能类型推断式返回（return关键字后使用） CTRL+ALT+V 自动生成表达式返回值 CTRL+ALT+M 重构表达式为函数(光标在表达式末尾) CTRL+Space 快速返回实现（return关键字后使用） 3、复制快捷方式 CTRL+D 复制行 CTRL+X 剪切,删除行4、其他快捷方式 CIRL+U 大小写切换 CTRL+Z 倒退 CTRL+SHIFT+Z 向前 CTRL+ALT+F12 资源管理器打开文件夹 SHIFT+ALT+INSERT 竖编辑模式 Ctrl+/ 将当前行代码注释或取消注释 Ctrl+Shift+/ 在当前光标位置添加 /*/ 或将 /xxxx*/ 内容取消注释 CTRL+W 选中代码，连续按会有其他效果 ALT+ ←/→ 切换代码视图 CTRL+ALT ←/→ 返回上次编辑的位置 ALT+ ↑/↓ 在方法间快速移动定位 ALT+1 快速打开或隐藏工程面板 CTRL+SHIFT+UP/DOWN 代码向上/下移动。 CTRL+UP/DOWN 光标跳转到第一行或最后一行下 ESC 光标返回编辑框 SHIFT+ESC 光标返回编辑框,关闭无用的窗口 ALT+J 相同单词多选 CTRL+SHIFT+A 工具调用 Ctrl+PageUp/PageDown 光标跳转到第一行或最后一行]]></content>
      <categories>
        <category>goland</category>
      </categories>
      <tags>
        <tag>golang</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python面试题]]></title>
    <url>%2F2019%2F11%2F21%2Fpython%E9%9D%A2%E8%AF%95%E9%A2%98%2F</url>
    <content type="text"><![CDATA[spark的有几种部署模式，每种模式特点？ 本地模式 Spark不一定非要跑在hadoop集群，可以在本地，起多个线程的方式来指定。方便调试，本地模式分三类 local：只启动一个executor local[k]: 启动k个executor local：启动跟cpu数目相同的 executor standalone模式 分布式部署集群，自带完整的服务，资源管理和任务监控是Spark自己监控，这个模式也是其他模式的基础 Spark on yarn模式 分布式部署集群，资源和任务监控交给yarn管理 粗粒度资源分配方式，包含cluster和client运行模式 cluster 适合生产，driver运行在集群子节点，具有容错功能 client 适合调试，dirver运行在客户端 Spark On Mesos模式 Spark技术栈有哪些组件，每个组件都有什么功能，适合什么应用场景？ Spark core 是其它组件的基础，spark的内核 主要包含：有向循环图、RDD、Lingage、Cache、broadcast等 SparkStreaming 是一个对实时数据流进行高通量、容错处理的流式处理系统 将流式计算分解成一系列短小的批处理作业 Spark sql： 能够统一处理关系表和RDD，使得开发人员可以轻松地使用SQL命令进行外部查询 MLBase 是Spark生态圈的一部分专注于机器学习，让机器学习的门槛更低 MLBase分为四部分：MLlib、MLI、ML Optimizer和MLRuntime。 GraphX 是Spark中用于图和图并行计算 spark有哪些组件 master：管理集群和节点，不参与计算。 worker：计算节点，进程本身不参与计算，和master汇报。 Driver：运行程序的main方法，创建spark context对象。 spark context：控制整个application的生命周期，包括dagsheduler和task scheduler等组件。 client：用户提交程序的入口。 Spark section-1 Spark运行细节(13) spark工作机制 用户在client端提交作业后，会由Driver运行main方法并创建spark context上下文。 执行add算子，形成dag图输入dagscheduler 按照add之间的依赖关系划分stage输入task scheduler task scheduler会将stage划分为taskset分发到各个节点的executor中执行 Spark应用程序的执行过程 构建Spark Application的运行环境（启动SparkContext） SparkContext向资源管理器（可以是Standalone、Mesos或YARN）注册并申请运行Executor资源； 资源管理器分配Executor资源，Executor运行情况将随着心跳发送到资源管理器上； SparkContext构建成DAG图，将DAG图分解成Stage，并把Taskset发送给Task Scheduler Executor向SparkContext申请Task，Task Scheduler将Task发放给Executor运行，SparkContext将应用程序代码发放给Executor。 Task在Executor上运行，运行完毕释放所有资源。 driver的功能是什么？ 一个Spark作业运行时包括一个Driver进程，也是作业的主进程，具有main函数，并且有SparkContext的实例，是程序的人口点； 功能： 向集群申请资源 负责了作业的调度和解析 生成Stage并调度Task到Executor上（包括DAGScheduler，TaskScheduler） Spark中Work的主要工作是什么？ 管理当前节点内存，CPU的使用状况，接收master分配过来的资源指令，通过ExecutorRunner启动程序分配任务 worker就类似于包工头，管理分配新进程，做计算的服务，相当于process服务 worker不会运行代码，具体运行的是Executor是可以运行具体appliaction写的业务逻辑代码 task有几种类型？2种 resultTask类型，最后一个task shuffleMapTask类型，除了最后一个task都是 什么是shuffle，以及为什么需要shuffle？ shuffle中文翻译为洗牌，需要shuffle的原因是：某种具有共同特征的数据汇聚到一个计算节点上进行计算 Spark master HA 主从切换过程不会影响集群已有的作业运行，为什么？ 因为程序在运行之前，已经申请过资源了，driver和Executors通讯，不需要和master进行通讯的。 Spark并行度怎么设置比较合适 spark并行度，每个core承载2~4个partition（并行度） 并行读和数据规模无关，只和内存和cpu有关 Spaek程序执行，有时候默认为什么会产生很多task，怎么修改默认task执行个数？ 有很多小文件的时候，有多少个输入block就会有多少个task启动 spark中有partition的概念，每个partition都会对应一个task，task越多，在处理大规模数据的时候，就会越有效率 Spark中数据的位置是被谁管理的？每个数据分片都对应具体物理位置，数据的位置是被blockManager管理 为什么要进行序列化减少存储空间，高效存储和传输数据，缺点：使用时需要反序列化，非常消耗CPU Spark如何处理不能被序列化的对象？封装成object Spark提交你的jar包时所用的命令是什么？spark-submit Spark section-2 Spark 与 Hadoop/MapReduce 比较(7) Mapreduce和Spark的相同和区别 两者都是用mr模型来进行并行计算 hadoop的一个作业：job job分为map task和reduce task，每个task都是在自己的进程中运行的 当task结束时，进程也会结束 spark用户提交的任务：application 一个application对应一个sparkcontext，app中存在多个job 每触发一次action操作就会产生一个job 这些job可以并行或串行执行 每个job中有多个stage，stage是shuffle过程中DAGSchaduler通过RDD之间的依赖关系划分job而来的 每个stage里面有多个task，组成taskset有TaskSchaduler分发到各个executor中执行 executor的生命周期是和app一样的，即使没有job运行也是存在的，所以task可以快速启动读取内存进行计算。 hadoop的job只有map和reduce操作，表达能力比较欠缺 在mr过程中会重复的读写hdfs，造成大量的io操作，多个job需要自己管理关系。 spark的迭代计算都是在内存中进行的 API中提供了大量的RDD操作如join，groupby等 通过DAG图可以实现良好的容错 简答说一下hadoop的mapreduce编程模型 首先map task会从本地文件系统读取数据，转换成key-value形式的键值对集合，使用的是hadoop内置的数据类型（longwritable、text） 将键值对集合输入mapper进行业务处理过程，将其转换成需要的key-value在输出 之后会进行一个partition分区操作，默认使用的是hashpartitioner，自定义分区：重写getpartition方法 之后会对key进行进行sort排序，grouping分组操作将相同key的value合并分组输出 之后进行一个combiner归约操作，其实就是一个本地段的reduce预处理，以减小后面shufle和reducer的工作量 reduce task会通过网络将各个数据收集进行reduce处理 最后将数据保存或者显示，结束整个job 简单说一下hadoop和spark的shuffle相同和差异？ high-level 角度： 两者并没有大的差别 都是将 mapper（Spark: ShuffleMapTask）的输出进行 partition，不同的 partition 送到不同的 reducer（Spark 里 reducer 可能是下一个 stage 里的 ShuffleMapTask，也可能是 ResultTask） Reducer 以内存作缓冲区，边 shuffle 边 aggregate 数据，等到数据 aggregate 好以后进行 reduce()。 low-level 角度： Hadoop MapReduce 是 sort-based，进入 combine() 和 reduce() 的 records 必须先 sort。 好处：combine/reduce() 可以处理大规模的数据 因为其输入数据可以通过外排得到 mapper 对每段数据先做排序 reducer 的 shuffle 对排好序的每段数据做归并 Spark 默认选择的是 hash-based，通常使用 HashMap 来对 shuffle 来的数据进行 aggregate，不提前排序 如果用户需要经过排序的数据：sortByKey() 实现角度： Hadoop MapReduce 将处理流程划分出明显的几个阶段：map(), spilt, merge, shuffle, sort, reduce() Spark 没有这样功能明确的阶段，只有不同的 stage 和一系列的 transformation()，spill, merge, aggregate 等操作需要蕴含在 transformation() 中 简单说一下hadoop和spark的shuffle过程 hadoop：map端保存分片数据，通过网络收集到reduce端 spark：spark的shuffle是在DAGSchedular划分Stage的时候产生的，TaskSchedule要分发Stage到各个worker的executor，减少shuffle可以提高性能 partition和block的关联 hdfs中的block是分布式存储的最小单元，等分，可设置冗余，这样设计有一部分磁盘空间的浪费，但是整齐的block大小，便于快速找到、读取对应的内容 Spark中的partition是RDD的最小单元，RDD是由分布在各个节点上的partition组成的。 partition是指的spark在计算过程中，生成的数据在计算空间内最小单元 同一份数据（RDD）的partion大小不一，数量不定，是根据application里的算子和最初读入的数据分块数量决定 block位于存储空间；partion位于计算空间，block的大小是固定的、partion大小是不固定的，是从2个不同的角度去看数据。 Spark为什么比mapreduce快？ 基于内存计算，减少低效的磁盘交互 高效的调度算法，基于DAG 容错机制Linage Mapreduce操作的mapper和reducer阶段相当于spark中的哪几个算子？ 相当于spark中的map算子和reduceByKey算子，区别：MR会自动进行排序的，spark要看具体partitioner Spark section-3 RDD(4) RDD机制 分布式弹性数据集，简单的理解成一种数据结构，是spark框架上的通用货币 所有算子都是基于rdd来执行的 rdd执行过程中会形成dag图，然后形成lineage保证容错性等 从物理的角度来看rdd存储的是block和node之间的映射 RDD的弹性表现在哪几点？ 自动的进行内存和磁盘的存储切换； 基于Lingage的高效容错； task如果失败会自动进行特定次数的重试； stage如果失败会自动进行特定次数的重试，而且只会计算失败的分片； checkpoint和persist，数据计算之后持久化缓存 数据调度弹性，DAG TASK调度和资源无关 数据分片的高度弹性，a.分片很多碎片可以合并成大的，b.par RDD有哪些缺陷？ 不支持细粒度的写和更新操作（如网络爬虫） spark写数据是粗粒度的，所谓粗粒度，就是批量写入数据 （批量写） 但是读数据是细粒度的也就是说可以一条条的读 （一条条读） 不支持增量迭代计算，Flink支持 什么是RDD宽依赖和窄依赖？ RDD和它依赖的parent RDD(s)的关系有两种不同的类型窄依赖：每一个parent RDD的Partition最多被子RDD的一个Partition使用 （一父一子）宽依赖：多个子RDD的Partition会依赖同一个parent RDD的Partition （一父多子）Spark section-4 RDD操作(13)1. cache和pesist的区别 cache和persist都是用于缓存RDD，避免重复计算.cache() == .persist(MEMORY_ONLY) cache后面能不能接其他算子,它是不是action操作？ 可以接其他算子，但是接了算子之后，起不到缓存应有的效果，因为会重新触发cachecache不是action操作 什么场景下要进行persist操作？以下场景会使用persist 某个步骤计算非常耗时或计算链条非常长，需要进行persist持久化shuffle之后为什么要persist，shuffle要进性网络传输，风险很大，数据丢失重来，恢复代价很大shuffle之前进行persist，框架默认将数据持久化到磁盘，这个是框架自动做的。 rdd有几种操作类型？三种！！ transformation，rdd由一种转为另一种rddactioncronroller，控制算子(cache/persist) 对性能和效率的有很好的支持 reduceByKey是不是action？ 不是，很多人都会以为是action，reduce rdd是action collect功能是什么，其底层是怎么实现的？ driver通过collect把集群中各个节点的内容收集过来汇总成结果collect返回结果是Array类型的，合并后Array中只有一个元素，是tuple类型（KV类型的）的。 map与flatMap的区别 map：对RDD每个元素转换，文件中的每一行数据返回一个数组对象flatMap：对RDD每个元素转换，然后再扁平化，将所有的对象合并为一个对象，会抛弃值为null的值 列举你常用的action？collect，reduce,take,count,saveAsTextFile等 union操作是产生宽依赖还是窄依赖？ 窄依赖 Spark累加器有哪些特点？ 全局的，只增不减，记录全局集群的唯一状态在exe中修改它，在driver读取executor级别共享的，广播变量是task级别的共享两个application不可以共享累加器，但是同一个app不同的job可以共享 spark hashParitioner的弊端 分区原理：对于给定的key，计算其hashCode弊端是数据不均匀，容易导致数据倾斜 RangePartitioner分区的原理 尽量保证每个分区中数据量的均匀，而且分区与分区之间是有序的，也就是说一个分区中的元素肯定都是比另一个分区内的元素小或者大分区内的元素是不能保证顺序的简单的说就是将一定范围内的数映射到某一个分区内 Spark中的HashShufle的有哪些不足？ shuffle产生海量的小文件在磁盘上，此时会产生大量耗时的、低效的IO操作；容易导致内存不够用，由于内存需要保存海量的文件操作句柄和临时缓存信息容易出现数据倾斜，导致OOMSpark section-5 大数据问题(7) 如何使用Spark解决TopN问题？（互联网公司常面）https://blog.csdn.net/oopsoom/article/details/25815443 如何使用Spark解决分组排序问题？（互联网公司常面）https://blog.csdn.net/huitoukest/article/details/51273143 给定a、b两个文件，各存放50亿个url，每个url各占64字节，内存限制是4G，让你找出a、b文件共同的url? 方案1：可以估计每个文件安的大小为5G×64=320G，远远大于内存限制的4G。所以不可能将其完全加载到内存中处理。考虑采取分而治之的方法。遍历文件a，对每个url求取hash(url)%1000，然后根据所取得的值将url分别存储到1000个小文件(记为a0,a1,…,a999)中。这样每个小文件的大约为300M。遍历文件b，采取和a相同的方式将url分别存储到1000小文件(记为b0,b1,…,b999)。这样处理后，所有可能相同的url都在对应的小文件(a0vsb0,a1vsb1,…,a999vsb999)中，不对应的小文件不可能有相同的url。然后我们只要求出1000对小文件中相同的url即可。求每对小文件中相同的url时，可以把其中一个小文件的url存储到hash_set中。然后遍历另一个小文件的每个url，看其是否在刚才构建的hash_set中，如果是，那么就是共同的url，存到文件里面就可以了。方案2：如果允许有一定的错误率，可以使用Bloomfilter，4G内存大概可以表示340亿bit。将其中一个文件中的url使用Bloomfilter映射为这340亿bit，然后挨个读取另外一个文件的url，检查是否与Bloomfilter，如果是，那么该url应该是共同的url(注意会有一定的错误率)。 有一个1G大小的一个文件，里面每一行是一个词，词的大小不超过16字节，内存限制大小是1M，要求返回频数最高的100个词。 Step1：顺序读文件中，对于每个词x，取hash(x)%5000，然后按照该值存到5000个小文件(记为f0,f1,…,f4999)中，这样每个文件大概是200k左右，如果其中的有的文件超过了1M大小，还可以按照类似的方法继续往下分，直到分解得到的小文件的大小都不超过1M;Step2：对每个小文件，统计每个文件中出现的词以及相应的频率(可以采用trie树/hash_map等)，并取出出现频率最大的100个词(可以用含100个结点的最小堆)，并把100词及相应的频率存入文件，这样又得到了5000个文件;Step3：把这5000个文件进行归并(类似与归并排序); 现有海量日志数据保存在一个超级大的文件中，该文件无法直接读入内存，要求从中提取某天出访问百度次数最多的那个IP。分而治之+Hash1)IP地址最多有2^32=4G种取值情况，所以不能完全加载到内存中处理;2)可以考虑采用“分而治之”的思想，按照IP地址的Hash(IP)%1024值，把海量IP日志分别存储到1024个小文件中。这样，每个小文件最多包含4MB个IP地址;3)对于每一个小文件，可以构建一个IP为key，出现次数为value的Hashmap，同时记录当前出现次数最多的那个IP地址;4)可以得到1024个小文件中的出现次数最多的IP，再依据常规的排序算法得到总体上出现次数最多的IP; 在2.5亿个整数中找出不重复的整数，注，内存不足以容纳这2.5亿个整数。 方案1：采用2-Bitmap(每个数分配2bit，00表示不存在，01表示出现一次，10表示多次，11无意义)进行，共需内存2^32*2bit=1GB内存，还可以接受。然后扫描这2.5亿个整数，查看Bitmap中相对应位，如果是00变01，01变10，10保持不变。所描完事后，查看bitmap，把对应位是01的整数输出即可。方案2：也可采用与第1题类似的方法，进行划分小文件的方法。然后在小文件中找出不重复的整数，并排序。然后再进行归并，注意去除重复的元素。 腾讯面试题：给40亿个不重复的unsignedint的整数，没排过序的，然后再给一个数，如何快速判断这个数是否在那40亿个数当中? 申请512M的内存，一个bit位代表一个unsignedint值。读入40亿个数，设置相应的bit位，读入要查询的数，查看相应bit位是否为1，为1表示存在，为0表示不存在。Spark section-6 机器学习算法(4)1. mllib支持的算法？ 分类、聚类、回归、协同过滤 kmeans算法原理 随机初始化中心点范围，计算各个类别的平均值得到新的中心点。重新计算各个点到中心值的距离划分，再次计算平均值得到新的中心点，直至各个类别数据平均值无变化。 朴素贝叶斯分类算法原理对于待分类的数据和分类项，根据待分类数据的各个特征属性，出现在各个分类项中的概率判断该数据是属于哪个类别的。 关联规则挖掘算法apriori原理 一个频繁项集的子集也是频繁项集，针对数据得出每个产品的支持数列表，过滤支持数小于预设值的项，对剩下的项进行全排列，重新计算支持数，再次过滤，重复至全排列结束，可得到频繁项和对应的支持数。Spark section-7 Hive(2)1. Hive中存放是什么？ 表（数据+元数据） 存的是和hdfs的映射关系，hive是逻辑上的数据仓库，实际操作的都是hdfs上的文件，HQL就是用sql语法来写的mr程序。2. Hive与关系型数据库的关系？ 没有关系，hive是数据仓库，不能和数据库一样进行实时的CURD操作。是一次写入多次读取的操作，可以看成是ETL工具。Spark服务端口8080 spark集群web ui端口4040 sparkjob监控端口18080 jobhistory端口Spark Job 默认的调度模式 - FIFORDD 特点 - 可分区/可序列化/可持久化Broadcast - 任何函数调用/是只读的/存储在各个节点Accumulator - 支持加法/支持数值类型/可并行Task 数量由 Partition 决定Task 运行在 Workder node 中 Executor 上的工作单元master 和 worker 通过 Akka 方式进行通信的默认的存储级别 - MEMORY_ONLYhive 的元数据存储在 derby 和 MySQL 中有什么区别 - 多会话DataFrame 和 RDD 最大的区别 - 多了 schema]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv笔记]]></title>
    <url>%2F2019%2F10%2F30%2Fopencv%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[一 安装部署环境 https://blog.csdn.net/cwt19902010/article/details/78887388 下载 https://www.lfd.uci.edu/~gohlke/pythonlibs/ opencv_python‑4.1.2‑cp37‑cp37m‑win_amd64.whl 方法1：通过cmd命令窗口： 执行命令：pip install opencv-python 方法二 推荐第二种 https://www.cnblogs.com/ylzj/p/8041810.html pip3 install opencv-python 出现连接A后再服务器上 wget A 这样下载快很多 同理 pip3 install -U numpy pip3 install pillow pip3 install opencv-contrib-python 人脸识别训练 https://www.cnblogs.com/xp12345/p/9818435.html]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql笔记]]></title>
    <url>%2F2019%2F10%2F17%2Fmysql%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[一 常用命令 先连接mysql终端 A服务器连接B服务器的mysql mysql -h rm-m5e9u8q5la8y35m8y.mysql.rds.aliyuncs.com -u ebeasy_psro_r -p ebsy_pro_db 1.desc eb_jinli_report_one; 查看表结构 2.show create table users\G; 建表语句 3.DROP TABLE table_name ; 删除表 4.truncate table table_name 清空表 5.source /home/xiaoayong/aa.sql 执行sql文件,文件位于A服务器 在A服务器root用户下 ##带条件导出数据 mysqldump --column-statistics=0 -h rm-m5e9u8q5la8y35m8y.mysql.rds.aliyuncs.com -u ebeasy_psro_r -p ebstat_pro_db ebsy_pro_db --where=&quot;type=1 and value=0&quot; &gt; /home/xiaoayong/eb_gionee_report_zero2.sql 在输入密码 导出到A服务器 /home/xiaoayong目录下 6.更新 UPDATE eb_jinli_report_one SET zsykq_lv = ROUND((zsykqdj_uv/dbtabdj_uv)*100,2), xqyssfh_pv = 1884739, xqyssfh_uv=105950 where type=0 and date_url=20190905; 7.创建视图 CREATE VIEW `订单数据` AS SELECT id `id`, d `日期`, c `订单数`, CASE p WHEN 2 THEN &apos;blibli&apos; WHEN 3 THEN &apos;tokopiedia&apos; WHEN 4 THEN &apos;bukalapak&apos; WHEN 5 THEN &apos;lazada&apos; ELSE &apos;all&apos; END AS `平台名称`, g `订单金额(gmv)`, r `总佣金`, r `预计返现`, u `订单用户数`, s `客单价`, a `人均下单率` FROM eb_report_order ORDER BY stat_date DESC,id DESC;]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python3学习笔记]]></title>
    <url>%2F2019%2F10%2F12%2Fpython3%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[一：测试环境 https://github.com/kon9chunkit/GitHub-Chinese-Top-Charts cmd 输入python 即可开始学习教程 1：深入 Python 流程控制 1) if … elif … elif else 2) Python 的 for 语句依据任意序列（链表或字符串）中的子项，按它们在序列中的顺序来进行迭代 3) 如果你需要一个数值序列，内置函数 range() 会很方便，它生成一个等差级数链表: 4) break 和 continue 语句, 以及循环中的 else 子句 5) pass 语句 函数 关键字 def 引入了一个函数 定义。在其后必须跟有函数名和包括形式参数的圆括号。函数体语句从下一行开始，必须是缩进的。 函数体的第一行语句可以是可选的字符串文本，这个字符串是函数的文档字符串，或者称为 docstring。 在 Python 中，你也可以定义包含若干参数的函数。这里有三种可用的形式，也可以混合使用。 4.7.1. 默认参数值 4.7.2. 关键字参数 4.7.3. 可变参数列表 4.7.4. 参数列表的分拆 4.7.5. Lambda 形式]]></content>
      <categories>
        <category>python3</category>
      </categories>
      <tags>
        <tag>python3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[metabase使用笔记]]></title>
    <url>%2F2019%2F09%2F06%2Fmetabase%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[一 安装 https://github.com/metabase/metabase 安装完填好数据库后 二 使用 可以先创建视图 ROP VIEW v_jinli_three_taobao; CREATE VIEW v_jinli_three_taobao AS SELECT stat_date, tcxs_pv, jhdj_pv FROM eb_jinli_report_three WHERE type=1; DROP VIEW `v_jinli_three_pinduoduo`; CREATE VIEW `v_jinli_three_pinduoduo` AS SELECT stat_date, tcxs_pv, jhdj_pv FROM eb_jinli_report_three WHERE type=2; DROP VIEW `v_jinli_four_all`; CREATE VIEW `v_jinli_four_all` AS SELECT stat_date, bgqd_pv, tcxs_pv, tcjh_pv, dbtabzs_pv, dbtabdj_pv, fwgwzsy_pv, zsykqdj_pv FROM eb_jinli_report_four WHERE type=0; DROP VIEW `金立输出PV表`; CREATE VIEW `金立输出PV表` AS SELECT f.stat_date `日期`, bgqd_pv `帮购app打开PV`, f.tcxs_pv `帮购首页助手弹窗弹出PV`, tcjh_pv `帮购首页助手弹窗点击PV`, dbtabzs_pv `帮购底部助手tab展示PV`, dbtabdj_pv `帮购底部助手tab点击PV`, t.tcxs_pv `淘宝首页助手弹窗弹出PV`, t.jhdj_pv `淘宝首页助手弹窗点击PV`, p.tcxs_pv `拼多多首页助手弹窗弹出PV`, p.jhdj_pv `拼多多首页助手弹窗点击PV`, fwgwzsy_pv `访问助手开启页PV`, zsykqdj_pv `助手开关开启点击PV` FROM v_jinli_four_all AS f, v_jinli_three_taobao as t, v_jinli_three_pinduoduo as p WHERE f.stat_date = t.stat_date AND f.stat_date = p.stat_date ORDER BY f.stat_date; 在终端执行这些命令 然后登陆浏览器metabase admin重新打开窗口同步数据 即可看到刚才创建的视图 save你想要的视图 save到你想放在分类下]]></content>
      <categories>
        <category>sql db</category>
      </categories>
      <tags>
        <tag>metabase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[golang笔记]]></title>
    <url>%2F2019%2F08%2F13%2Fgolang%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[常用知识点一 部署 windows下取别的名字 SET CGO_ENABLED=0 //这个不需要 SET GOOS=windows go build -o proxy-win-amd64.exe main.go start /b proxy-win-amd64.exe linux下取别的名字 SET CGO_ENABLED=0 //这个不需要 SET GOOS=linux go build -o proxy-linux-amd64 main.go nohup proxy-linux-amd64 server --mode=dev &amp; nohup proxy-linux-amd64 &amp; 二 debug goland 点击 debug go build main.go 查看console输出 查看debugger窗口输出 具体错误信息 三 golang 整形转字符串 strconv.Itoa(a) golang字符串分割成数组遍历 itemidString := &quot;sss,aaa,bb&quot; chrstr := strings.Split(itemidString, &quot;,&quot;) if len(chrstr) != 0 { for _, va := range chrstr { //va就是sss } } 字符串转float64 strconv.ParseFloat(value.Pay_price, 64) 四 golang调试 fmt.Printf(“v1 type:%T\n”, value.Discount) //打印变量类型 beego.Info(&quot;sssssaaa&quot;,value.Discount) //记录日志 五 panic说明 panic日志中，打印出来的确实是输入参数的值；如果函数有返回值，则返回值也会打印； 但实际上是以字长word来打印的(word:操作系统处理信息的基本单位)； 而每个参数占多少word，又和参数的类型有关(所以我们才会有参数个数对不上的疑惑)。 类型与字长 golang中基础类型如int、char、byte等的字长和C语言一致，不再展开， 下面列举常用的几个： 指针占一个word； int占1个word string占两个word (一个指向不可变字符数组的指针，一个string的长度)； 切片占三个word (一个指向底层数组的指针，一个切片的长度，一个切片的容量) 接口占两个word(一个指向实际类型的指针，一个指向数据的指针) 更详细的可参考：https://research.swtch.com/godata 堆栈解析 有了上面的知识储备， 本文开头提到的panic信息就能解释通了。 第一个参数slice []string，因为切片类型占3个word，所以： slice := make([]string, 2, 4) // 该切片的实际值 Pointer: 0xc000078f48 Length: 0x2 Capacity: 0x4 // 定义 func Fun1(slice []string, t *Test, i int) // 堆栈 main.Fun1(0x2080c3f50, 0x2, 0x4, 0x0, 0x7) 第二个参数t *Test，因为指针占1个word，所以： 实际调用 Fun1(slice, nil, 7) // 定义 func Fun1(slice []string,t *Test, i int) // 堆栈 main.Fun1(0x2080c3f50, 0x2, 0x4,0x0, 0x7) 因此从这里也能看出传入的t是nil，这也是panic的所在； 第三个参数i int，因为int占1个word，所以： //定义 func Fun1(slice []string, t *Test,i int) //堆栈 main.Fun1(0x2080c3f50, 0x2, 0x4, 0x0,0x7) 函数有返回值 这里增加一个 有两个返回值的函数Fun2： 再看堆栈信息： 由上述第2行可以清晰看出，Fun2有两个返回值(int, error)， 堆栈日志中main.Fun2就增加了0x1056c1d, 0xc000078f88, 0x1004c30三个值，其中： int占一个word，对应0x1056c1d； error是interface，占两个word，对应0xc000078f88, 0x1004c30； 函数到方法 将上述的Fun2改成如下方式并调用： 修改前后堆栈日志对比： 由上可知，两者唯一差别是，func (m *M) Fun2 堆栈的第一个是 m的地址，其他的和 func Func2 一致。 引申问题 参数个数限制 进一步发现堆栈中的Fun2最多只能有10个参数，当有更多时候，会用...省略掉： 参数packing 前面说了，堆栈参数是以word为单位来打印的，那如果参数不足word长度呢，如bool，char等？还是很有趣的，请看下文： Fun1的堆栈日志： 很明显：b1、b2、b3、c占用了一个word，对应0x19010001，这就是参数packing； 因此可知：堆栈参数中的高位对应着右边的参数，地位对应着左边的参数。 六 go命令参数 在 build, clean, get, install, list, run, test中 -o output 指定编译输出的名称，代替默认的包名。 -i install 安装作为目标的依赖关系的包(用于增量编译提速)。 -a 完全编译，不理会-i产生的.a文件(文件会比不带-a的编译出来要大？) -n 仅打印输出build需要的命令，不执行build动作（少用）。 -p n 开多少核cpu来并行编译，默认为本机CPU核数（少用）。 -race 同时检测数据竞争状态，只支持 linux/amd64, freebsd/amd64, darwin/amd64 和 windows/amd64. -msan 启用与内存消毒器的互操作。仅支持linux / amd64，并且只用Clang / LLVM作为主机C编译器（少用）。 -v 打印出被编译的包名（少用）. -work 打印临时工作目录的名称，并在退出时不删除它（少用）。 -x 同时打印输出执行的命令名（-n）（少用）. -asmflags &apos;flag list&apos; 传递每个go工具asm调用的参数（少用） -buildmode mode 编译模式（少用） &apos;go help buildmode&apos; -compiler name 使用的编译器 == runtime.Compiler (gccgo or gc)（少用）. -gccgoflags &apos;arg list&apos; gccgo 编译/链接器参数（少用） -gcflags &apos;arg list&apos; 垃圾回收参数（少用）. -installsuffix suffix a suffix to use in the name of the package installation directory, in order to keep output separate from default builds. If using the -race flag, the install suffix is automatically set to race or, if set explicitly, has _race appended to it. Likewise for the -msan flag. Using a -buildmode option that requires non-default compile flags has a similar effect. -ldflags &apos;flag list&apos; &apos;-s -w&apos;: 压缩编译后的体积 -s: 去掉符号表 -w: 去掉调试信息，不能gdb调试了 -linkshared 链接到以前使用创建的共享库 -buildmode=shared. -pkgdir dir 从指定位置，而不是通常的位置安装和加载所有软件包。例如，当使用非标准配置构建时，使用-pkgdir将生成的包保留在单独的位置。 -tags &apos;tag list&apos; 构建出带tag的版本. -toolexec &apos;cmd args&apos; a program to use to invoke toolchain programs like vet and asm. For example, instead of running asm, the go command will run &apos;cmd args /path/to/asm &lt;arguments for asm&gt;&apos;. 七 golang语法 变量的并行申明和赋值 如果你想要交换两个变量的值，则可以简单地使用 a, b = b, a，两个变量的类型必须是相同。 指定变量类型，如果没有初始化，则变量默认为零值。 数值类型（包括complex64/128）为 0；布尔类型为 false；字符串为 &quot;&quot;（空字符串）；其他类型为 nil。 所有像 int、float、bool 和 string 这些基本类型都属于值类型，使用这些类型的变量直接指向存在内存中的值： 当使用等号 = 将一个变量的值赋值给另一个变量时，如：j = i，实际上是在内存中将 i 的值进行了拷贝： 你可以通过 &amp;i 来获取变量 i 的内存地址，例如：0xf840000040（每次的地址都可能不一样）。值类型的变量的值存储在栈中。 内存地址会根据机器的不同而有所不同，甚至相同的程序在不同的机器上执行后也会有不同的内存地址。因为每台机器可能有不同的存储器布局，并且位置分配也可能不同。 更复杂的数据通常会需要使用多个字，这些数据一般使用引用类型保存。 一个引用类型的变量 r1 存储的是 r1 的值所在的内存地址（数字），或内存地址中第一个字所在的位置。 这个内存地址称之为指针，这个指针实际上也被存在另外的某一个值中。 同一个引用类型的指针指向的多个字可以是在连续的内存地址中（内存布局是连续的），这也是计算效率最高的一种存储形式；也可以将这些字分散存放在内存中，每个字都指示了下一个字所在的内存地址。 当使用赋值语句 r2 = r1 时，只有引用（地址）被复制。 如果 r1 的值被改变了，那么这个值的所有引用都会指向被修改后的内容，在这个例子中，r2 也会受到影响。 常量中的数据类型只可以是布尔型、数字型（整数型、浮点型和复数）和字符串型。 iota，特殊常量，可以认为是一个可以被编译器修改的常量。 iota 在 const关键字出现时将被重置为 0(const 内部的第一行之前)，const 中每新增一行常量声明将使 iota 计数一次(iota 可理解为 const 语句块中的行索引)。 https://www.runoob.com/go/go-operators.html i=1&lt;&lt;0, j=3&lt;&lt;1（&lt;&lt; 表示左移的意思） 左移和右移都是位运算的概念。 左移一位相当于该数乘以2，左移2位相当于该数乘以2^2=4。但此结论只适用于该数左移时被溢出舍弃的高位中不包含1的情况。 右移一位相当于除2，右移n位相当于除以2的n次方。这里是取商哈，余数就不要了。 [左移] 丢弃最高位，0补最低位 左移是把一个数按照二进制每位向左移动若干位，在c语言中用运算符 &lt;&lt; 表示。 [右移] 丢弃最低位，符号补高位 右移的概念和左移正好相反，是指的向右移动若干位，在c语言中用运算符 &gt;&gt; 表示。 i=1：左移 0 位,不变仍为 1; j=3：左移 1 位,变为二进制 110, 即 6; 3对应的二进制是 11 按位异或运算符&quot;^&quot;是双目运算符。 其功能是参与运算的两数各对应的二进位相异或，当两对应的二进位相异时，结果为1。 ^ 标识相异则1 &amp; 返回变量存储地址 * 指针变量。 指针变量 * 和地址值 &amp; 的区别：指针变量保存的是一个地址值，会分配独立的内存来存储一个整型数字。当变量前面有 * 标识时，才等同于 &amp; 的用法，否则会直接输出一个整型数字。 break 语句 经常用于中断当前 for 循环或跳出 switch 语句 continue 语句 跳过当前循环的剩余语句，然后继续进行下一轮循环。 goto 语句 将控制转移到被标记的语句。 select 是 Go 中的一个控制结构，类似于用于通信的 switch 语句。每个 case 必须是一个通信操作，要么是发送要么是接收。 select 随机执行一个可运行的 case。如果没有 case 可运行，它将阻塞，直到有 case 可运行。一个默认的子句应该总是可运行的。 无限循环 for true { fmt.Printf(&quot;这是无限循环。\n&quot;); } https://www.runoob.com/go/go-functions.html 函数参数 函数如果使用参数，该变量可称为函数的形参。 形参就像定义在函数体内的局部变量。 调用函数，可以通过两种方式来传递参数： 值传递 值传递是指在调用函数时将实际参数复制一份传递到函数中，这样在函数中如果对参数进行修改，将不会影响到实际参数。 Go 语言中数组可以存储同一类型的数据，但在结构体中我们可以为不同项定义不同的数据类型。 一个指针变量指向了一个值的内存地址。 类似于变量和常量，在使用指针前你需要声明指针。指针声明格式如下： 如果一个指针变量存放的又是另一个指针变量的地址，则称这个指针变量为指向指针的指针变量。 当定义一个指向指针的指针变量时，第一个指针存放第二个指针的地址，第二个指针存放变量的地址 递归，就是在运行的过程中调用自己。 Go 语言提供了另外一种数据类型即接口，它把所有的具有共性的方法定义在一起，任何其他类型只要实现了这些方法就是实现了这个接口。 Go 允许使用 go 语句开启一个新的运行期线程， 即 goroutine，以一个不同的、新创建的 goroutine 来执行一个函数。 同一个程序中的所有 goroutine 共享同一个地址空间。 通道（channel）是用来传递数据的一个数据结构。 通道可用于两个 goroutine 之间通过传递一个指定类型的值来同步运行和通讯。操作符 &lt;- 用于指定通道的方向，发送或接收。如果未指定方向，则为双向通道。 ch &lt;- v // 把 v 发送到通道 ch v := &lt;-ch // 从 ch 接收数据 // 并把值赋给 v 什么是可变参数函数？ 可变参数函数即其参数数量是可变的 —— 0 个或多个。声明可变参数函数的方式是在其参数类型前带上省略符（三个点）前缀。 0x00 前提 对可变参数不了解的同学，可以先看这篇文章可变参数终极指南 0x01 第一个坑 不能通过空接口类型向可变参数传递一个普通的切片 ，需要将普通切片转换为空接口切片 0x02 第二个坑 可变参数不支持确定参数和slice解包的混合。 所谓匿名函数，就是没有名字的函数 匿名函数的两种使用方式 一、在定义匿名函数的时候就可以直接使用（这种方式只使用一次） package main import ( &quot;fmt&quot; ) func main(){ res1 := func (n1 int, n2 int) int { return n1 + n2 }(10, 30) //括号里的10,30 就相当于参数列表，分别对应n1和n2 fmt.Println(&quot;res1=&quot;,res1) } 二、将匿名函数赋给一个变量（函数变量），再通过该变量来调用匿名函数 全局匿名函数 全局匿名函数就是将匿名函数赋给一个全局变量，那么这个匿名函数在当前程序里可以使用 //Test_fun 就是定义好的全局变量 //全局变量必须首字母大写 var ( Test_fun = func (n1 int, n2 int) int { return n1 - n2 } ) func main(){ val1 := Test_fun(9, 7) fmt.Println(&quot;val1=&quot;, val1) } 在 Go 中数组是值类型而不是引用类型。这意味着当数组变量被赋值时，将会获得原数组（译者注：也就是等号右面的数组）的拷贝。新数组中元素的改变不会影响原数组中元素的值。同样的，如果将数组作为参数传递给函数，仍然是值传递，在函数中对（作为参数传入的）数组的修改不会造成原数组的改变. 数组中括号里面有东西 a := [3]int{12, 78, 50} // shorthand declaration to create array c := []int{6, 7, 8} 当若干个切片共享同一个底层数组时，对每一个切片的修改都会反映在底层数组中 切片保留对底层数组的引用。只要切片存在于内存中，数组就不能被垃圾回收。这在内存管理方便可能是值得关注的。假设我们有一个非常大的数组，而我们只需要处理它的一小部分，为此我们创建这个数组的一个切片，并处理这个切片。这里要注意的事情是，数组仍然存在于内存中，因为切片正在引用它。 可以看到切片包含长度、容量、以及一个指向首元素的指针。当将一个切片作为参数传递给一个函数时，虽然是值传递，但是指针始终指向同一个数组。因此将切片作为参数传给函数时，函数对该切片的修改在函数外部也可以看到。 解决该问题的一个方法是使用 copy 函数 func copy(dst, src []T) int 来创建该切片的一个拷贝。这样我们就可以使用这个新的切片，原来的数组可以被垃圾回收。 12 defer 延迟对函数进行调用； 即时对函数的参数进行求值； 根据 defer 顺序反序调用； 1 defer 函数参数即时求值 也就是说 defer 函数会被延迟调用，但传递给 defer 函数的参数会在 defer 语句处就被准备好。 2 反序调用 可以看出f函数返回时，第一个 defer 函数最后被执行，而最后一个 defer 函数却第一个被执行。 defer 主要用于简化编程（以及实现 panic/recover ，后面会专门写一篇相关文章来介绍） defer 实现了函数的延迟调用； defer 使用要点：延迟调用，即时求值和反序调用； go 语言的 return 会被编译器翻译成多条指令，其中包括保存返回值，调用defer注册的函数以及实现函数返回。]]></content>
      <categories>
        <category>golang</category>
      </categories>
      <tags>
        <tag>golang</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hadoop/spark笔记]]></title>
    <url>%2F2019%2F08%2F13%2Fhadoop-spark%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[这里是centos6.8上安装单机版spark 一. 安装JDK 二. 安装spark。 tar -zxvf spark-2.3.1-bin-hadoop2.7.tgz mv spark-2.3.1-bin-hadoop2.7 spark-2.3.1 vim /etc/profile export SPARK_HOME=/opt/spark-2.3.1 export PATH=$PATH:$SPARK_HOME/bin source /etc/profile cd /opt/spark-2.3.1/conf/ cp spark-env.sh.template spark-env.sh vim spark-env.sh export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.181-3.b13.el7_5.x86_64 #这里是你jdk的安装路径 export SPARK_HOME=/opt/spark-2.3.1 export SPARK_MASTER_IP=XXX.XX.XX.XXX #将这里的xxx改为自己的Linux的ip地址 source spark-env.sh cd /opt/spark-2.3.1/conf/ cp slaves.template slaves vim slaves 在文件的最后加上如下配置： localhost source slaves cd sbin/ ./start-all.sh ./start-master.sh ./start-slave.sh spark://node1:7077 或者 pyspark sparkShell 三 centos环境部署python脚本 统计日志并写入xls 安装xlwt 一 安装pip curl -O https://bootstrap.pypa.io/get-pip.py python get-pip.py 二 安装xlrd--读 xlwt--写 xlrd：是python从excel读数据的第三方控件； xlwt：是python从excel写数据的第三方控件； xlutils：是python使用xlrd、xlwt的工具箱。若安装不成功，可能原因是需要安装setuptools。 pip install xlrd pip install xlwt pip install xlutils 基于日志算每个字段的值 cat * | grep &apos;EVENT_GOODS_DETAIL_PV&apos; | wc -l cat ${log_path} | grep -E &apos; &quot;nm&quot;:&quot;SQG_TAOBAO_SEARCH_ACCURATE_RESULT_CLICK&quot; | &quot;nm&quot;:&quot;SQG_TAOBAO_SEARCH_SIMILAR_RESULT_CLICK&quot; &apos; | wc -l //专区入口点击人数 根据m1去重 第三列3,3 cat * | grep &apos;&quot;nm&quot;:&quot;JINGANG_CLICK_EVERYTIME&quot;&apos; | grep &apos;&quot;name&quot;:&quot;大额神券&quot;&apos; | uniq | sort -t &apos;,&apos; -k 3,3 -u | wc -l //打印符合条件的m1，在第四列 cat -n * | grep &apos; &quot;nm&quot;:&quot;USER_START_APK&quot; &apos; | awk -F: &apos;{print $4}&apos; //也可以用jq cat * | jq &apos;. | {nm:.JINGANG_CLICK_EVERYTIME,name:.淘宝白菜}&apos; | wc -l 具体脚本代码----python 注意python for循环缩进 #!/usr/bin/env python # coding=utf-8 from pyspark import SparkContext from xlwt import * #import xlwt file=Workbook(encoding = &apos;utf-8&apos;) #指定file以utf-8的格式打开 table=file.add_sheet(&apos;baobiaoone&apos;) sc=SparkContext(appName=&quot;jinlifour&quot;) # sc.setLogLevel(&quot;ERROR&quot;) sc.setLogLevel(&quot;INFO&quot;) rdd=sc.textFile(&quot;file:///home/xiaoayong/work/jinli_qingdianshang/20190818&quot;) # rdd = sc.textFile(&apos;../csv/csv1/*&apos;) #print(rdd.count()) numAs=rdd.filter(lambda s:&apos;&quot;nm&quot;:&quot;GIONEE_OPEN_BANGO&quot;&apos; in s).count() numAsa=rdd.filter(lambda s:&apos;&quot;nm&quot;:&quot;GIONEE_BANGO_HELPER_DIALOG_SHOW&quot;&apos; in s).count() numAsb=rdd.filter(lambda s:&apos;&quot;nm&quot;:&quot;GIONEE_BANGO_HELPER_DIALOG_ACTIVATE_CLICK&quot;&apos; in s).count() numAsc=rdd.filter(lambda s:&apos;&quot;nm&quot;:&quot;GIONEE_BANGO_HELPER_DIALOG_COUSTOM_BACKGROUND&quot;&apos; in s).count() data={&quot;1&quot;:[&quot;张三&quot;,numAs,numAsa,100],&quot;2&quot;:[&quot;李四&quot;,numAsb,numAsc,95],&quot;3&quot;:[&quot;王五&quot;,60,66,68]} title = [&quot;学号&quot;,&quot;姓名&quot;,&quot;语文成绩&quot;,&quot;数学成绩&quot;,&quot;英语成绩&quot;,&quot;总分&quot;,&quot;平均分&quot;] for i in range(len(title)): # 循环列 table.write(0,i,title[i]) # 将title数组中的字段写入到0行i列中 for line in data: # 循环字典 print(&apos;line:&apos;,line) table.write(int(line),0,line) # 将line写入到第int(line)行，第0列中 summ = data[line][2] + data[line][3] # 成绩总分 table.write(int(line),5,summ) # 总分 table.write(int(line),6,summ/3) # 平均分 for i in range(len(data[line])): table.write(int(line),i+1,data[line][i]) file.save(&apos;baobiaoone.xls&apos;) //窗口统计 pyspark textFile=sc.textFile(&quot;hdfs://nameservice1/songshu/track/zheng_shihui/20190813/&quot;) textFile.count() spark.read.format(&quot;json&quot;).load(&quot;file:///home/xiaoayong/work/jinli_qingdianshang/20190818&quot;).createOrReplaceTempView(&quot;ds&quot;) spark.read.format(&quot;json&quot;).load(&quot;hdfs://nameservice1/songshu/track/jinli_qingdianshang/` date -d &apos;yesterday&apos; +&apos;%Y%m%d&apos; ` &quot;).createOrReplaceTempView(&quot;ds&quot;) spark.sql(&quot;SELECT * FROM ds&quot;).show() spark.sql(&quot;SELECT * FROM ds where nm=&apos;GIONEE_OPEN_BANGO&apos; &quot;).count() rdd = sc.textFile(&quot;file:///home/xiaoayong/work/jinli_qingdianshang/20190818&quot;) numAs=rdd.filter(lambda s:&apos;GIONEE_OPEN_BANGO&apos; in s).count() print(numAs) //crontab 10 01 * * * bash /home/xiaoayong/work/script/everyday.sh 1&gt;/home/xiaoayong/work/script/everyday.log 2&gt;&amp;1 25 02 * * * bash /home/xiaoayong/work/script/jinlireportone.sh 1&gt;/home/xiaoayong/work/script/jinlione.log 2&gt;&amp;1 0 14 * * * user/bin/python /home/xiaoayong/work/script/gethdfslog.py 1&gt;/home/xiaoayong/work/script/everyday.log 2&gt;&amp;1 0 14 * * * /opt/spark-2.1.0/bin/spark-submit bbb.py 1&gt;/home/xiaoayong/work/script/everyday.log 2&gt;&amp;1 链接网址 https://blog.csdn.net/yzh_1346983557/article/details/81624708 //单机版部署安装 http://spark.apache.org/ //官网 http://spark.apache.org/docs/latest/sql-programming-guide.html //学习sql http://spark.apache.org/docs/latest/quick-start.html //这个好像是入门的 https://www.cnblogs.com/xyf9575/p/7448613.html 第二次在 15号机重装 15 号docker安装pyspark 已经安装了 java jdk 和 python2.7.5 cd /data/server/pineddwget http://mirror.bit.edu.cn/apache/spark/spark-2.4.3/spark-2.4.3-bin-hadoop2.7.tgzwget https://downloads.lightbend.com/scala/2.13.0/scala-2.13.0.tgz tar zxvf spark-2.4.3-bin-hadoop2.7.tgzmv spark-2.4.3-bin-hadoop2.7 spark-2.4.3tar zxvf scala-2.13.0.tgzvim /etc/profile 加入export SPARK_HOME=/data/server/pinedd/spark-2.4.3export PATH=$SPARK_HOME/bin:$PATHexport PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.7-src.zip:$PYTHONPATH source /etc/profilepyspark 安装EPEL源：yum install epel-release 安装完EPEL源后，可以查看下jq包是否存在：yum list jq 安装jq：yum install jq yum install -y sendemail //shell 安装邮箱发送curl -O https://bootstrap.pypa.io/get-pip.py //安装pippython get-pip.py //很慢 二 安装xlrd–读 xlwt–写xlrd：是python从excel读数据的第三方控件；xlwt：是python从excel写数据的第三方控件；xlutils：是python使用xlrd、xlwt的工具箱。若安装不成功，可能原因是需要安装setuptools。pip install xlrdpip install xlwtpip install xlutils cp spark-env.sh.template spark-env.sh #export SCALA_HOME=/opt/scala-2.13.0export JAVA_HOME=/data/program/jdkexport SPARK_HOME=/data/server/pinedd/spark-2.4.3export SPARK_MASTER_IP=1.1.1.39 #export SPARK_EXECUTOR_MEMORY=512Mexport SPARK_WORKER_MEMORY=2Gexport SPARK_MASTER_PORT=7077 vim /etc/hosts1.1.1.39 pinedd-001 cp slaves.template slavesvim slaves在文件的最后加上如下配置：localhost 三 windows7使用Intellij IDEA编辑器运行pyspark 1) new project 选择Python 2) 在Project SDK中 NEW...选择运行环境 Add Python Interpreter（增加python 编译） 这四个都是可以跑的 1 Virtualenv Environment Python的虚拟环境可以使一个Python程序拥有独立的库library和解释器interpreter，而不用与其他Python程序共享统一个library和interpreter。虚拟环境的好处是避免了不同Python程序间的互相影响.例如程序A需要某个库的1.0版本，而程序B需要同样这个库的2.0版本，如果程序B执行则A就不能执行了。见formatlog里的venv 2 Conda Environment 3 System Interpreter system interpreter表示本地的解释器 4 Pipenv Environment pip 是 Python 最常用的包管理器，该工具提供了对Python 包的查找、下载、安装、卸载的功能。它能自动处理依赖 如果说venv是虚拟环境管理器，pip是包管理器，那么conda则是两者的结合。遗憾的是conda的包管理器做的一般，大多数时候还是使用pip安装包。pip只能安装Python的包，conda可以安装一些工具软件，即使这些软件不是基于Python开发的。conda虚拟环境是独立于操作系统解释器环境的，即无论操作系统解释器什么版本（哪怕2.7），我也可以指定虚拟环境python版本为3.6（见文章开头所说原博客），而venv是依赖主环境的。对于科学计算和大数据领域的人，conda是环境自动集成了numpy这样的主流科学计算包的，venv每个包都要自行下载。conda有图形化环境管理器，venv没有。（虽然开发人员几乎不用图形界面conda）。之前安装的anaconda3就是很强大。 有conda就是一个包管理工具和安装工具，他就是要做比pip更多的事情；在python-site-packages之外管理python 库依赖关系。 而且conda同样也像virtualenv一样创建一个虚拟环境。conda可以让你同时管理安装处理你有关python的任务和跟python无关的任务conda使用了一个新的包格式，你不能交替使用pip 和conda。因为pip不能安装和解析conda的包格式。你可以使用两个工具 但是他们是不能交互的。Anaconda是一个包含180+的科学包及其依赖项的发行版本。其包含的科学包包括：conda, numpy, scipy, ipython notebook等。conda是包及其依赖项和环境的管理工具。适用语言：Python, R, Ruby, Lua, Scala, Java, JavaScript, C/C++, FORTRAN。适用平台：Windows, macOS, Linux pip是用于安装和管理软件包的包管理器。Python 3.4及后续版本：默认安装，命令为pip3。virtualenv：用于创建一个独立的Python环境的工具。 conda结合了pip和virtualenv的功能。 https://blog.csdn.net/sinat_28442665/article/details/86292354 https://blog.csdn.net/C_tommy/article/details/86706458 3）这里我选择的则是系统环境也就是自己装的python3.7.4。Interpreter（解释器）选择的是D:\Python37\python.exe Project name 输入formatlog 对应路径D:\GoWorkPath\src\formatlog finish 4) 进入项目目录-&gt; sshscript 打开下面的test.py 遇到 from pyspark import SparkContext 有红点 直接点击安装即可。然后选择文件debug 文件即可 5) 看程序用到的包 和 程序执行文件路径 ---大致可以确定是IDEA中python SDK配置的路径有误，依次点击：File–&gt;Project Structure–&gt;Platform Settings–&gt;SDKs–&gt;Python 3.7，发现Classpath下是空的，问题找到。File-&gt;Invalidate Caches/Restart; 四 mysql相关操作 pyspark的 Mysql写入 一.需要提交spark时候，加入mysql-connect jar bin/spark-submit –master yarn-client –num-executors 5 –executor-cores 20 –executor-memory 20g –jars /usr/hdp/2.4.2.0-258/sqoop/lib/mysql-connector-java.jar /opt/spark-2.1.0-bin-hadoop2.7/dig.py 备注下，执行的python程序次序不能错。不然居然找不到mysql驱动。 二.写入的python程序如下： df.write.jdbc(url=&quot;jdbc:mysql://10.0.0.03:3306/test&quot; &quot;?user=test&amp;password=test123&quot;, mode=&quot;append&quot;, table=&quot;test&quot;, properties={&quot;driver&quot;: &apos;com.mysql.jdbc.Driver&apos;}) 备注下: 1.df是要写入的数据集，类似于R中的dataframe 2.mode是表明是否追加数据，否则创建新表 3.table指定插入的mysql表 4.url是mysql的链接参数。 bin/spark-submit --master yarn-client --num-executors 5 --executor-cores 20 --executor-memory 20g --jars /usr/hdp/2.4.2.0-258/sqoop/lib/mysql-connector-java.jar /opt/spark-2.1.0-bin-hadoop2.7/dig.py 五 阿里oss的相关操作 which pip 使用which命令查看Pip路径 Python2查看pip安装的软件包名称及版本 python2 -m pip list Python3查看pip安装的软件包及版本 python3 -m pip list pip freeze 例如查看 beautifulsoup4的安装路径 pip show beautifulsoup4 yum install python-devel pip install pycryptodome pip install oss2]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>hadoop spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[php面试题一]]></title>
    <url>%2F2019%2F07%2F25%2Fphp%E9%9D%A2%E8%AF%95%E9%A2%98%E4%B8%80%2F</url>
    <content type="text"><![CDATA[1 写一个函数，获取一篇文章内容中的全部图片，并下载 $html = file_get_contents($url); preg_match_all(‘/&lt;img[^&gt;]src=”([^”])”[^&gt;]&gt;/i’,$html, $matchs); //2、什么是CSRF攻击？XSS攻击？如何防范？ 防范方式: CSRF TOKEN, 即提交表单时同时提交一段由服务端渲染表单时生成的token,通过校验token来防范csrf攻击 简单来说,XSS就是正常页面执行了用户或黑客提交的前端代码,比如你用了eval(‘这里执行了用户提交的代码’), 或者你的页面正常解析了用户提交的html代码,如用户提交的个人信息是:window.href=”恶意网站连接”, 而你不加过滤转义就入库,然后页面正常解析html代码,最终用户访问这个页面就会跳转到恶意网站 ,这就是XSS 防范方式: 过滤&amp;&amp;转义用户输入(如htmlentities、htmlspecialchars),永久不要信任客户端3 应用中我们经常会遇到在user表随机调取10条数据来展示的情况，简述你如何实现该功能。 select from user where rand() limit 10;4 MYSQL中主键与唯一索引的区别 主键是一种约束，唯一索引是一种索引，两者在本质上是不同的 主键创建后一定包含一个唯一性索引，唯一性索引并不一定就是主键 唯一性索引列允许空值，而主键列不允许为空值 主键列在创建时，已经默认为空值 + 唯一索引了 主键可以被其他表引用为外键，而唯一索引不能 一个表最多只能创建一个主键，但可以创建多个唯一索引 主键更适合那些不容易更改的唯一标识，如自动递增列、身份证号等 在 RBO 模式下，主键的执行计划优先级要高于唯一索引。 两者可以提高查询的速度5 http状态码及其含意 200 - 请求成功 301 - 资源（网页等）被永久转移到其它URL 404 - 请求的资源（网页等）不存在 500 - 内部服务器错误 300 种选择。请求的资源可包括多个位置，相应可返回一个资源特征与地址的列表用于用户终端（例如：浏览器）选择 302 临时移动。与301类似。但资源只是临时被移动。客户端应继续使用原有URI 400 客户端请求的语法错误，服务器无法理解 401 请求要求用户的身份认证 403 服务器理解请求客户端的请求，但是拒绝执行此请求 501 服务器不支持请求的功能，无法完成请求 502 充当网关或代理的服务器，从远端服务器接收到了一个无效的请求6 MySQL主从同步与主主同步 https://www.cnblogs.com/wade-lt/p/9008058.html MySQL内建的复制功能是构建大型，高性能应用程序的基础。将MySQL的数据分布到多个系统上去，这种分布的机制，是通过将mysql的某一台主机的数据复制到其它主机（slave）上，并重新执行一遍来实现。 复制过程中一个服务器充当主服务器，而一个或多个其它服务器充当从服务器。主服务器将更新写入二进制日志文件，并维护文件的一个索引以跟踪日志循坏，这些日志可以记录发送到从服务器的更新。当一个从服务器 连接主服务器时，它通知主服务器从服务器在日志中读取的最后一次成功更新的位置。从服务器接收从那时起发生的任何更新，然后封锁并等待主服务器通知的更新。需注意的是： 在进行mysql复制时，所有对复制中的表的更新必须在主服务器上进行。否则必须要小心，以避免用户对主服器上的表进行更新与对从服务器上的表所进行更新之间的冲突。 （1）mysql支持哪些复制 a.基于语句的复制：在主服务器上执行的sql语句，在从服务器上执行同样的语句。mysql默认采用基于语句的复制，效率边角高。一旦发现没法精确复制时，会自动选着基于行的复制。 b.基于行的复制：把改变的内容复制过去，而不是把命令在从服务器上执行一遍。从mysql 5.0开始支持 c.混合类型的复制：默认采用基于语句的复制，一旦发现基于语句的无法精确复制时，就会采用基于行的复制。 （2）mysql复制解决的问题 a.数据分布（data distribution） b.负载平衡（load balancing） c.数据备份（backup），保证数据安全 d.高可用性与容错行（high availability and failover） e.实现读写分离，缓解数据库压力 （3）mysql主从复制原理 master服务器将数据的改变记录二进制binlog日志，当master上的数据发生改变时，则将其改变写入二进制日志中；slave服务器会在一定时间间隔内对master二进制日志进行探测其是否发生改变， 如果发生改变，则开始一个I/O Thread请求master二进制事件，同时主节点为每个I/O线程启动一个dump线程，用于向其发送二进制事件，并保存至从节点本地的中继日志中，从节点将启动SQL线程从中继日志 中读取二进制日志，在本地重放，使得其数据和主节点的保持一致，最后I/O Thread和SQL Thread将进入睡眠状态，等待下一次被唤醒。 注意几点： 1–master将操作语句记录到binlog日志中，然后授予slave远程连接的权限（master一定要开启binlog二进制日志功能；通常为了数据安全考虑，slave也开启binlog功能）。 2–slave开启两个线程：IO线程和SQL线程。其中：IO线程负责读取master的binlog内容到中继日志relay log里；SQL线程负责从relay log日志里读出binlog内容，并更新到slave的数据库里，这样就能保证slave数据和 master数据保持一致了。 3–Mysql复制至少需要两个Mysql的服务，当然Mysql服务可以分布在不同的服务器上，也可以在一台服务器上启动多个服务。 4–Mysql复制最好确保master和slave服务器上的Mysql版本相同（如果不能满足版本一致，那么要保证master主节点的版本低于slave从节点的版本） 5–master和slave两节点间时间需同步7 MySQL InnoDB默认行级锁。行级锁都是基于索引的 行级锁变为表级锁情况如下： 1、如果一条SQL语句用不到索引是不会使用行级锁的，会使用表级锁把整张表锁住。 2、表字段进行变更。 3、进行整表查询。(没使用索引) 4、like语句查询的时候。(没使用索引)8 冒泡排序 (3)时间复杂度 1.如果我们的数据正序，只需要走一趟即可完成排序。所需的比较次数C和记录移动次数M均达到最小值，即：Cmin=n-1;Mmin=0;所以，冒泡排序最好的时间复杂度为O(n)。 2.如果很不幸我们的数据是反序的，则需要进行n-1趟排序。每趟排序要进行n-i次比较(1≤i≤n-1)，且每次比较都必须移动记录三次来达到交换记录位置。在这种情况下，比较和移动次数均达到最大值： Cmax = n(n-1)/2 = O(n2) Mmax = 3n(n-1) / 2 = O(n2) 综上所述：冒泡排序总的平均时间复杂度为：O(n2) ,时间复杂度和数据状况无关。 常用排序算法的时间复杂度 最差时间分析 平均时间复杂度 稳定度 空间复杂度 冒泡排序 O(n2) O(n2) 稳定 O(1) 快速排序 O(n2) O(nlog2n) 不稳定 O(log2n)~O(n) 选择排序 O(n2) O(n2) 稳定 O(1) 二叉树排序 O(n2) O(nlog2n) 不稳定 O(n) 插入排序 O(n2) O(n2) 稳定 O(1) 堆排序 O(nlog2n) O(nlog2n) 不稳定 O(1) 希尔排序 O O 不稳定 O(1) 常用的时间复杂度所耗费的时间从小到大依次是：O(1)&lt;O(logn)&lt;O(n)&lt;O(nlogn)&lt;O(n^2)&lt;O(n^3)&lt;O(2^n)&lt;O(n!)&lt;O(n^n)。随着问题规模n的不断增大，上述时间复杂度不断增大，算法的执行效率越低。9 redis面试题 https://msd.misuland.com/pd/3065794831805580868 https://blog.csdn.net/XiaoHanZuoFengZhou/article/details/103053726 https://github.com/ng1091/redis-interview10 mysql面试题 https://blog.csdn.net/XiaoHanZuoFengZhou/article/details/103034615 https://blog.csdn.net/XiaoHanZuoFengZhou/article/details/103034636 https://www.cnblogs.com/aishangJava/p/11821629.html https://www.jianshu.com/p/c1a4b69ff21b https://blog.csdn.net/SuLinXin/article/details/9347109511 网络面试题 https://blog.csdn.net/XiaoHanZuoFengZhou/article/details/10303411112 github面试题 https://github.com/yongxinz/back-end-interview https://github.com/SwordfallYeung/Interview_BigData https://github.com/duiying/php-notes]]></content>
      <categories>
        <category>php</category>
      </categories>
      <tags>
        <tag>php</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux常用命令]]></title>
    <url>%2F2019%2F07%2F25%2Flinux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[1 netstat -anp 显示系统端口使用情况 lsof -i :80 显示占用80端口的进程情况 uname -a 内核信息 cat /proc/interrupts 显示中断信息 netstat -ntlp //查看当前所有tcp端口 netstat -ntulp |grep 80 //查看所有80端口使用情况 -t (tcp) 仅显示tcp相关选项 -u (udp)仅显示udp相关选项 -n 拒绝显示别名，能显示数字的全部转化为数字 -l 仅列出在Listen(监听)的服务状态 -p 显示建立相关链接的程序名 netstat -an | grep 3306 //查看所有3306端口使用情况 telnet ip 80 方式测试远程主机80是否打开 netstat -nap | grep [pid] 查看某个进程占用的端口 2 查看文件内容 cat 可以加more 、less控制输出的内容的大小 cat a.text | more chown -R www:www timeapi 修改timeapi目录为www:www 递归子目录修改： chown -R tuxapp source/ 增加脚本可执行权限： chmod a+x myscript 文本替换 sed sed [options] &apos;command&apos; file(s) 首处替换 sed &apos;s/text/replace_text/&apos; file //替换每一行的第一处匹配的text 全局替换 sed &apos;s/text/replace_text/g&apos; file 移除空白行 sed &apos;/^$/d&apos; file sed &apos;s/book/books/&apos; file #替换文本中的字符串： sed &apos;s/book/books/g&apos; file sed &apos;/^$/d&apos; file #删除空白行 sed -i “s/oldstring/newstring/g” grep oldstring -rl path 比如，要将目录/modules下面所有文件中的zhangsan都修改成lisi，这样做： sed -i “s/zhangsan/lisi/g” `grep zhangsan -rl /modules` 例子 将hexo目录下dn-lbstatics.qbox.me替换为busuanzi.ibruce.info 确保阅读次数准确 sed -i &quot;s/dn-lbstatics.qbox.me/busuanzi.ibruce.info/g&quot; `grep dn-lbstatics.qbox.me -rl hexo/` sed -i “s/6/sk/g” grep 6 -rl /home/work/test/*.sh # 注意这里的 &quot; &amp; &quot; 符号，如果没有 “&amp;”，就会直接将匹配到的字符串替换掉 sed &apos;s/^/添加的头部&amp;/g&apos; #在所有行首添加 sed &apos;s/$/&amp;添加的尾部/g&apos; #在所有行末添加 sed &apos;2s/原字符串/替换字符串/g&apos; #替换第2行 sed &apos;$s/原字符串/替换字符串/g&apos; #替换最后一行 sed &apos;2,5s/原字符串/替换字符串/g&apos; #替换2到5行 sed &apos;2,$s/原字符串/替换字符串/g&apos; #替换2到最后一行 sed &apos;s/原字符串/替换字符串/2g&apos; #替换2到最后一行 tail -n 24 可以指定显示行数 查看文件的前多少行 head -n100 filename 3 数据流处理awk #统计日志中访问最多的10个IP awk &apos;{a[$1]++}END{for(i in a)print a[i],i|&quot;sort -k1 -nr|head -n10&quot;}&apos; access.log 消除重复行 sort unsort.txt | uniq 4 统计 wc wc [-clw][–help][–version][文件…] -c或–bytes或–chars 只显示Bytes数。 -l或–lines 只显示行数。 -w或–words 只显示字数。 –help 在线帮助。 –version 显示版本信息。 wc -l fileA // 统计A文件行数 wc -w file // 统计单词数 wc -c file // 统计字符数 5 hosts文件定时备份及开机恢复 加反斜杠 \cp 执行cp命令时不走alias：（注：推荐这个方法！） vim hostsbak.sh #！/usr/bin/bash export LANG=&quot;en_US.UTF-8&quot; # \cp -rf /etc/hosts /data/server/pinedd/work/hosts.bak vim hostsload.sh #！/usr/bin/bash export LANG=&quot;en_US.UTF-8&quot; # \cp -rf /data/server/pinedd/work/hosts.bak /etc/hosts crontab 里 10 01 * * * bash /data/server/pinedd/work/script/hostsbak.sh 1&gt;/data/server/pinedd/work/script/hostsbak.log 2&gt;&amp;1 在/etc/rc.d/rc.local中加入以下语句： 开机自动执行此目录的脚本 echo &quot;su - ricky -c &apos;/bin/bash /data/server/pinedd/work/script/hostsload.sh&apos;&quot; 1&gt;/data/server/pinedd/work/script/hostsload.log 2&gt;&amp;1 6 登录远端服务器后 sz aaa.txt 下载aaa.txt到本地 rz 选择aaa.txt 上传本地aaa.txt到远端 7 screen命令 screen [-AmRvx -ls -wipe][-d &lt;作业名称&gt;][-h &lt;行数&gt;][-r &lt;作业名称&gt;][-s ][-S &lt;作业名称&gt;] -A 将所有的视窗都调整为目前终端机的大小。 -d &lt;作业名称&gt; 将指定的screen作业离线。 -h &lt;行数&gt; 指定视窗的缓冲区行数。 -m 即使目前已在作业中的screen作业，仍强制建立新的screen作业。 -r &lt;作业名称&gt; 恢复离线的screen作业。 -R 先试图恢复离线的作业。若找不到离线的作业，即建立新的screen作业。 -s 指定建立新视窗时，所要执行的shell。 -S &lt;作业名称&gt; 指定screen作业的名称。 -v 显示版本信息。 -x 恢复之前离线的screen作业。 -ls或--list 显示目前所有的screen作业。 -wipe 检查目前所有的screen作业，并删除已经无法使用的screen作业。 screen -S yourname -&gt; 新建一个叫yourname的session screen -ls -&gt; 列出当前所有的session screen -r yourname -&gt; 回到yourname这个session screen -d yourname -&gt; 远程detach某个session screen -d -r yourname -&gt; 结束当前session并回到yourname这个session Ctrl+D 最小化，后台运行 用screen -wipe命令清除该会话 Attached为当前有会话连接，Detached为会话断开 screen -X -S 374744 quit 结束某个会话 root用户下exit就可以结束某个会话 常用的 在root用户下 screen -S yourname -&gt; 新建一个叫yourname的session screen -ls -&gt; 列出当前所有的session ctrl+a 然后 d挂起某个会话 8 grep命令详解 https://www.jianshu.com/p/4ec50fdaf388 grep or 操作符 1 使用 | grep ‘pattern1|pattern2’ filename grep ‘Tech|Sales’ employee.txt 2 使用选项 -E grep -E ‘pattern1|pattern2’ filename grep -E ‘Tech|Sales’ employee.txt 3 使用 egrep egrep ‘pattern1|pattern2’ filename egrep ‘Tech|Sales’ employee.txt 4 使用选项 -e grep -e pattern1 -e pattern2 filename grep -e Tech -e Sales employee.txt grep and 操作 1 使用 -E &apos;pattern1.*pattern2&apos; grep -E &apos;pattern1.*pattern2&apos; filename grep -E &apos;pattern1.*pattern2|pattern2.*pattern1&apos; filename 两个pattern的顺序不是固定的，可以是乱序的 grep -E &apos;Manager.*Sales|Sales.*Manager&apos; employee.txt 2 使用多个grep命令 grep -E &apos;pattern1&apos; filename | grep -E &apos;pattern2&apos; grep Manager employee.txt | grep Sales grep not 操作 1 使用选项 grep -v grep -v &apos;pattern1&apos; filename 2 可以将NOT操作与其他操作联合起来，以此实现更强大的功能 组合。 egrep &apos;Manager|Developer&apos; employee.txt | grep -v Sales 10 awk命令 https://www.cnblogs.com/wangbaihan/p/9262296.html awk 是行处理器，优点是处理庞大文件时不会出现内存溢出或处理缓慢的问题，通常用来格式化文本信息。awk依次对每一行进行处理，然后输出。 命令形式 awk [-F|-f|-v] ‘BEGIN{} //{command1; command2} END{}’ file [-F|-f|-v] 大参数，-F指定分隔符，-f调用脚本，-v定义变量 var=value ’ ’ 引用代码块 BEGIN 初始化代码块，在对每一行进行处理之前，初始化代码，主要是引用全局变量，设置FS分隔符 // 匹配代码块，可以使字符串或正则表达式 {} 命令代码块，包含一条或多条命令 ;多条命令使用分号分隔 END 结尾代码块，对每一行进行处理后再执行的代码块，主要进行最终计算或输出 由于篇幅限制，可自行查找更详细的信息。这里awk命令的作用是从文件中每一行取出我们需要的字符串 sort 命令 sort将文件的每一行作为一个单位，相互比较，比较原则是从首字符向后，依次按ASCII码值进行比较，最后将他们按升序输出。 -b：忽略每行前面开始出的空格字符； -c：检查文件是否已经按照顺序排序； -d：排序时，处理英文字母、数字及空格字符外，忽略其他的字符； -f：排序时，将小写字母视为大写字母； -i：排序时，除了040至176之间的ASCII字符外，忽略其他的字符； -m：将几个排序号的文件进行合并； -M：将前面3个字母依照月份的缩写进行排序； -n：依照数值的大小排序； -o&lt;输出文件&gt;：将排序后的结果存入制定的文件； -r：以相反的顺序来排序； -t&lt;分隔字符&gt;：指定排序时所用的栏位分隔字符； uniq 命令 uniq 命令用于报告或忽略文件中的重复行，一般与sort命令结合使用 -c或——count：在每列旁边显示该行重复出现的次数； -d或–repeated：仅显示重复出现的行列； -f&lt;栏位&gt;或–skip-fields=&lt;栏位&gt;：忽略比较指定的栏位； -s&lt;字符位置&gt;或–skip-chars=&lt;字符位置&gt;：忽略比较指定的字符； -u或——unique：仅显示出一次的行列； -w&lt;字符位置&gt;或–check-chars=&lt;字符位置&gt;：指定要比较的字符。 &gt; 命令 &gt; 是定向输出到文件,如果文件不存在，就创建文件。如果文件存在，就将其清空 另外 &gt;&gt;是将输出内容追加到目标文件中。其他同&gt; 现有日志文件11.txt内容 需要分组统计nm的量 {&quot;ba&quot;:&quot;GIONEE&quot;,&quot;ip&quot;:&quot;111.85.164.152&quot;,&quot;m1&quot;:&quot;5232a3e61b8a6da97718c841919fbd21&quot;,&quot;mo&quot;:&quot;GIONEE GN5007&quot;,&quot;nm&quot;:&quot;SQG_TODAY_FIRST_OPEN&quot;,&quot;p&quot;:&quot;4&quot;,&quot;tm&quot;:1568559600,&quot;v&quot;:&quot;2.2.0&quot;} {&quot;ba&quot;:&quot;GIONEE&quot;,&quot;ip&quot;:&quot;106.9.163.80&quot;,&quot;m1&quot;:&quot;d181e463d4519d6ef87aa94975dfc6a2&quot;,&quot;mo&quot;:&quot;GIONEE M7&quot;,&quot;nm&quot;:&quot;SQG_TODAY_FIRST_OPEN&quot;,&quot;p&quot;:&quot;1&quot;,&quot;tm&quot;:1568559600,&quot;v&quot;:&quot;2.2.0&quot;} {&quot;ba&quot;:&quot;GIONEE&quot;,&quot;ip&quot;:&quot;113.6.185.218&quot;,&quot;m1&quot;:&quot;f73a98d4af94f14be333a30784e28a58&quot;,&quot;mo&quot;:&quot;GN5005&quot;,&quot;nm&quot;:&quot;BANGO_WAKE_UP_SQG_HELPER&quot;,&quot;p&quot;:&quot;1&quot;,&quot;tm&quot;:1568559600,&quot;v&quot;:&quot;2.2.0&quot;} {&quot;ba&quot;:&quot;GIONEE&quot;,&quot;ip&quot;:&quot;171.114.50.161&quot;,&quot;m1&quot;:&quot;5232a3e61b8a6da97718c841919fbd21&quot;,&quot;mo&quot;:&quot;GIONEE S10&quot;,&quot;nm&quot;:&quot;SQG_TODAY_FIRST_OPEN&quot;,&quot;p&quot;:&quot;1&quot;,&quot;tm&quot;:1568559600,&quot;v&quot;:&quot;2.2.0&quot;} {&quot;ba&quot;:&quot;GIONEE&quot;,&quot;ip&quot;:&quot;182.38.202.42&quot;,&quot;m1&quot;:&quot;92e697ffc4e2998d2b4a3aa6f80528c1&quot;,&quot;mo&quot;:&quot;GIONEE S10&quot;,&quot;nm&quot;:&quot;BANGO_WAKE_UP_SQG_HELPER&quot;,&quot;p&quot;:&quot;9&quot;,&quot;tm&quot;:1568559600,&quot;v&quot;:&quot;2.2.0&quot;} {&quot;ba&quot;:&quot;GIONEE&quot;,&quot;ip&quot;:&quot;221.9.36.175&quot;,&quot;m1&quot;:&quot;92e697ffc4e2998d2b4a3aa6f80528c1&quot;,&quot;mo&quot;:&quot;GIONEE S10&quot;,&quot;nm&quot;:&quot;BANGO_WAKE_UP_SQG_HELPER&quot;,&quot;p&quot;:&quot;1&quot;,&quot;tm&quot;:1568559600,&quot;v&quot;:&quot;2.2.0&quot;} {&quot;ba&quot;:&quot;GIONEE&quot;,&quot;ip&quot;:&quot;123.114.54.186&quot;,&quot;m1&quot;:&quot;43fc6f1d584e39bfd0b88b0b5af94028&quot;,&quot;mo&quot;:&quot;GIONEE S10&quot;,&quot;nm&quot;:&quot;BANGO_WAKE_UP_SQG_HELPER&quot;,&quot;p&quot;:&quot;1&quot;,&quot;tm&quot;:1568559600,&quot;v&quot;:&quot;2.2.0&quot;} 从每一行取出我们需要的字符串 cat 11.txt | awk -F &apos;(&quot;nm&quot;:&quot;|&quot;,&quot;p&quot;)&apos; &apos;{print $2}&apos; 输出 SQG_TODAY_FIRST_OPEN SQG_TODAY_FIRST_OPEN BANGO_WAKE_UP_SQG_HELPER SQG_TODAY_FIRST_OPEN BANGO_WAKE_UP_SQG_HELPER BANGO_WAKE_UP_SQG_HELPER BANGO_WAKE_UP_SQG_HELPER awk 命令中 -F 指定每一行的分隔符 在这里 ‘(txt=|&amp;client)’是分隔符，它是一个正则表达式。意义是，用’txt=’或’&amp;client’ 作为分隔符。 在这里会被分成1,2,3三个部分 第二部分我们需要。而’{print $2}’的意思是将每行得到的第二个值打印出来，$0代表获取的所有值 对行进行排序 先排序是因为去重与统计的 ‘uniq’命令只能处理相邻行 cat 11.txt | awk -F &apos;(&quot;nm&quot;:&quot;|&quot;,&quot;p&quot;)&apos; &apos;{print $2}&apos; | sort 统计数量与去重 cat 11.txt | awk -F &apos;(&quot;nm&quot;:&quot;|&quot;,&quot;p&quot;)&apos; &apos;{print $2}&apos; | sort | uniq -c uniq -c 中的-c 代表在每列旁边显示该行重复出现的次数 按重复次数排序 sort 的 -n：依照数值的大小排序；-r 按照相反顺序排列 4 BANGO_WAKE_UP_SQG_HELPER 3 SQG_TODAY_FIRST_OPEN 将结果输出到文件中 cat 11.txt | awk -F &apos;(&quot;nm&quot;:&quot;|&quot;,&quot;p&quot;)&apos; &apos;{print $2}&apos; | sort | uniq -c | sort -nr &gt; count.txt 去重统计总数 不是分组统计 输出5 cat 11.txt | awk -F &apos;(&quot;m1&quot;:&quot;|&quot;,&quot;mo&quot;)&apos; &apos;{print $2}&apos; | sort | uniq -c | wc -l 满足nm=SQG_TODAY_FIRST_OPEN 且根据m1去重 cat 11.txt | grep &apos;&quot;nm&quot;:&quot;SQG_TODAY_FIRST_OPEN&quot;&apos; | awk -F &apos;(&quot;m1&quot;:&quot;|&quot;,&quot;mo&quot;)&apos; &apos;{print $2}&apos; | sort | uniq -c | wc -l 等同如下jq写法 cat 11.txt | grep &apos;&quot;nm&quot;:&quot;SQG_TODAY_FIRST_OPEN&quot;&apos; | jq -r .m1 | sort | uniq -c | wc -l ##找出nm=EVENT_PASTE_SEARCH_PV 且appid=10004的pv量 cat * | grep EVENT_PASTE_SEARCH_PV | jq .seg.custom.appid | grep 10004 | wc -l ##找出nm=EVENT_PASTE_SEARCH_PV 且appid=10004的uv量,根据m1去重 cat * | grep EVENT_PASTE_SEARCH_PV | grep &apos;&quot;appid&quot;:10004&apos; | jq .m1 | sort | uniq -c | wc -l 第二种方法 先逗号分割，得到的结果 再：分割 cat * | awk -F , &apos;{print $5}&apos; | awk -F : &apos;{print $2}&apos; | sort | uniq -c | sort -nr 第三种方法jq cat 1.txt | jq .nm | sort | uniq -c 直接jq取.nm的值 cat * | jq -r &apos;.nm&apos; | grep BANGO_WAKE_UP_SQG_HELPER |wc -l //统计nm为BANGO_WAKE_UP_SQG_HELPER的个数 cat * | jq -r -c &apos;. | {nm:.nm,p:.p}&apos; | grep SQG_TODAY_FIRST_OPEN | grep &apos;&quot;p&quot;:&quot;2&quot;&apos; //满足nm=SQG_TODAY_FIRST_OPEN且p=2的 cat * | jq -r -c &apos;. | {nm:.nm,p:.p}&apos; | sort | uniq -c | sort -nr cat * | awk -F &apos;(&quot;nm&quot;:&quot;|&quot;,&quot;p&quot;)&apos; &apos;{print $2}&apos; | sort | uniq -d -d或--repeated 仅显示重复出现的行 -u或--unique 仅显示出现一次的行 单条日志如下： {&quot;ba&quot;:&quot;GIONEE&quot;,&quot;ip&quot;:&quot;112.98.134.22&quot;,&quot;m1&quot;:&quot;0d8578b9e18eeffe0af469f4dbbb5e9b&quot;,&quot;mo&quot;:&quot;GIONEES10C&quot;,&quot;nm&quot;:&quot;AUTO_UPLOAD_USE_TIME_LIST_DATA&quot;,&quot;p&quot;:&quot;1&quot;,&quot;seg&quot;:{&quot;custom&quot;:{&quot;appv&quot;:&quot;3.5.1&quot;,&quot;data&quot;:[&quot;ALIVE_MYSELF@@3@@1570541425477&quot;,&quot;OPEN@@2@@1570549769170&quot;,&quot;OPEN@@2@@1570549869956&quot;,&quot;OPEN@@2@@1570549945797&quot;,&quot;OPEN@@4@@1570549989446&quot;,&quot;OPEN@@2@@1570550009438&quot;,&quot;OPEN@@6@@1570550108061&quot;,&quot;OPEN@@2@@1570550171468&quot;,&quot;OPEN@@2@@1570550250246&quot;,&quot;OPEN@@2@@1570550323524&quot;,&quot;OPEN@@4@@1570550365150&quot;,&quot;OPEN@@2@@1570550404568&quot;,&quot;OPEN@@2@@1570550461831&quot;,&quot;OPEN@@2@@1570550516939&quot;,&quot;OPEN@@7@@1570550614604&quot;],&quot;from&quot;:&quot;gionee&quot;}},&quot;tm&quot;:1570550624,&quot;v&quot;:&quot;2.2.0&quot;} 一：统计活跃PV的旧版拉活的出现OPEN@@的日志条数 cat * | grep AUTO_UPLOAD_USE_TIME_LIST_DATA | grep OPEN@@ | wc -l 二：活跃PV的旧版拉活的出现OPEN@@的次数，一条日志里出现多次。用OPEN@@分割得出每条日志出现的次数，然后累加 cat * | grep AUTO_UPLOAD_USE_TIME_LIST_DATA | grep OPEN@@ | awk -FOPEN@@ &apos;{print NF-1}&apos; | awk &apos;{sum+=$1} END {print sum}&apos; NF 表示的是浏览记录的域的个数 $NF 表示的最后一个Field（列），即输出最后一个字段的内容 NF其实是number of field, 即整行(或者说record)里面词 (更准确的翻译应该是域)的总数 $(NF-1) 就是倒数第二个词 扩展 grep &apos;17:[0-9][0-9]:&apos; awk.sh | awk -F, &apos;{if(int($19)&gt;=0)print $10}&apos; | sort | uniq -c | sort -n 这句-F, 表示根据逗号分割 分割后第19行大于0 -n表示正序排列 cat * | awk -F &apos;(&quot;p&quot;:&quot;|&quot;,&quot;tm&quot;)&apos; &apos;{sum += $2} END {print sum}&apos; 统计p的和 11 jq命令 https://www.jianshu.com/p/3522fe70de19 https://www.linuxidc.com/Linux/2017-10/148037.htm https://www.jianshu.com/p/851015d1b186 安装EPEL源： yum install epel-release 安装 yum install jq yum install -y jq cat * | jq &apos;. | {nm:.JINGANG_CLICK_EVERYTIME,name:.淘宝白菜}&apos; | wc -l cat * | jq &apos;.[0] | {nm:.name,city:.address.city}&apos; jq支持使用map()或者map_values()遍历列表，或者对象的值。 例如 echo &apos;{&quot;a&quot;:1,&quot;b&quot;:2,&quot;c&quot;:3}&apos; | jq &apos;map_values(1 + .)&apos; echo &apos;[1,2,3]&apos; | jq &apos;map(1 + .)&apos; 删除某个key jq &apos;del(filter)&apos; json.data key 是用来获取JSON中的 key 元素的： cat json_raw.txt | jq &apos;keys&apos; 检查某个key是否存在 cat json_raw.txt | jq &apos;has(&quot;name&quot;)&apos; 合并 jq -n &apos;{a:&quot;test&quot;} + {b:2}&apos; 删除 cat test.json |jq &apos;del(.b)&apos; 更新 cat test.json |jq &apos;.b=&quot;testb&quot;&apos; 查询 cat test.json |jq &apos;. + {d:4}&apos; |jq &apos;.d={dd:5}&apos; |jq .d.dd //{ &quot;a&quot;: &quot;test&quot;, &quot;b&quot;: 2, &quot;c&quot;: &quot;testc&quot;, &quot;d&quot;: { &quot;dd&quot;: 5 } } 查看数据类型 echo &quot;{}&quot; |jq -r type echo &apos;[0, false, [], {}, null, &quot;hello&quot;]&apos; |jq &apos;map(type)&apos; 查询数组中的值 echo [1,2,3] |jq .[1] 查询数组长度 echo [1,2,3,9] |jq &apos;.|length&apos; 数组相加 echo [1,2,3] |jq &apos;. + [4,5,6]&apos; 高级查询 echo [1,2,3] | jq &apos;map(select(. &gt;= 2))&apos; echo [1,2,3] | jq &apos;map(select(. != 2))&apos; 类型转换 echo &apos;[&quot;a&quot;,&quot;b,c,d&quot;,&quot;e&quot;]&apos; |jq &apos;join(&quot;,&quot;)&apos; jq -R &apos;split(&quot;,&quot;)|{&quot;name&quot;:.[0],&quot;age&quot;:.[1],&quot;sex&quot;:.[2]}&apos; ./test.json 计算元素长度，对于对象，length表示对象里的元素个数，对于string，length表示string字符数，对于列表，表示列表元素个数 jq &apos;.age | length&apos; 1.txt sort -t &apos;,&apos; -k 1,1 -u 其中 -t 指定列之间的分隔符， -k 指定从第几列到第几列作为去重标准 -c或--count 在每列旁边显示该行重复出现的次数 -d或--repeated 仅显示重复出现的行 -f&lt;栏位&gt;或--skip-fields=&lt;栏位&gt; 比较时跳过前n列，从n+1列开始比较 -s&lt;字符位置&gt;或--skip-chars=&lt;字符位置&gt;比较时跳过前n个字符，从n+1个字符开始比较 -u或--unique 仅显示出现一次的行 -w&lt;字符位置&gt;或--check-chars=&lt;字符位置&gt;对每行第n个字符以后的内容不作对照 最简单的jq程序是表达式&quot;.&quot;，它不改变输入，但可以将其优美地输出，便于阅读和理解。cat json.txt | jq &apos;.&apos; 输出列表中的第一个元素，可以使用[index]： cat json.txt | jq &apos;.[0]&apos; curl -X http://localhost:8080/a/b | jq . Some of the options include: -c compact instead of pretty-printed output; -n use `null` as the single input value; -e set the exit status code based on the output; -s read (slurp) all inputs into an array; apply filter to it; -r output raw strings, not JSON texts; -R read raw strings, not JSON texts; //输出SQG_TODAY_FIRST_OPEN 没有双引号 -C colorize JSON; -M monochrome (don&apos;t colorize JSON); -S sort keys of objects on output; --tab use tabs for indentation; --arg a v set variable $a to value &lt;v&gt;; --argjson a v set variable $a to JSON value &lt;v&gt;; --slurpfile a f set variable $a to an array of JSON texts read from &lt;f&gt;; 12 lsof命令 #列出所有打开的文件: lsof 备注: 如果不加任何参数，就会打开所有被打开的文件，建议加上一下参数来具体定位 # 查看谁正在使用某个文件 lsof /filepath/file #递归查看某个目录的文件信息 lsof +D /filepath/filepath2/ 备注: 使用了+D，对应目录下的所有子目录和文件都会被列出 # 比使用+D选项，遍历查看某个目录的所有文件信息 的方法 lsof | grep ‘/filepath/filepath2/’ # 列出某个用户打开的文件信息 lsof -u username 备注: -u 选项，u其实是user的缩写 # 列出某个程序所打开的文件信息 lsof -c mysql 备注: -c 选项将会列出所有以mysql开头的程序的文件，其实你也可以写成lsof | grep mysql,但是第一种方法明显比第二种方法要少打几个字符了 # 列出多个程序多打开的文件信息 lsof -c mysql -c apache # 列出某个用户以及某个程序所打开的文件信息 lsof -u test -c mysql # 列出除了某个用户外的被打开的文件信息 lsof -u ^root 备注：^这个符号在用户名之前，将会把是root用户打开的进程不让显示 # 通过某个进程号显示该进行打开的文件 lsof -p 1 # 列出多个进程号对应的文件信息 lsof -p 123,456,789 # 列出除了某个进程号，其他进程号所打开的文件信息 lsof -p ^1 # 列出所有的网络连接 lsof -i # 列出所有tcp 网络连接信息 lsof -i tcp # 列出所有udp网络连接信息 lsof -i udp # 列出谁在使用某个端口 lsof -i :3306 # 列出谁在使用某个特定的udp端口 lsof -i udp:55 # 特定的tcp端口 lsof -i tcp:80 # 列出某个用户的所有活跃的网络端口 lsof -a -u test -i # 列出所有网络文件系统 lsof -N #域名socket文件 lsof -u #某个用户组所打开的文件信息 lsof -g 5555 # 根据文件描述列出对应的文件信息 lsof -d description(like 2) # 根据文件描述范围列出文件信息 lsof -d 2-3]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[php常用命令]]></title>
    <url>%2F2019%2F07%2F25%2Fphp%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[1、PHP运行指定文件 php -f test.php (-f 可省略) 2、命令行直接运行PHP代码 php -r “phpinfo();” 如果结果太长，还可以 php -r “phpinfo();” | less 分页展示 3、交互模式运行PHP php -a php &gt; echo (2+3); 5 control + c/z 或者 exit 退出交互模式 4、PHP脚本作为shell脚本运行 没有权限则切换到root用户 sudo su echo ‘#!/usr/bin/php\n&lt;?php var_dump($argv); ?&gt;’ &gt; phpscript #累计数据快照 1 0 * * * flock -xn /home/rong/crontab/UnderRepaymentTimeoutdone.lock -c &apos;/usr/bin/php /home/rong/www/time-order/webroot/command.php HistorySnapshot InsertHistoryData&apos; 5、其他常用命令 php -m 内置及Zend加载的模块 php -i 等价于 phpinfo() php -i | grep php.ini 查看php配置文件加载路径 php –ini 同上 php -v 查看php版本 php –version 同上 php –re 查看是否安装相应的扩展 如 php –re gd 6 php -S 这个内置的Web服务器主要用于本地开发使用，不可用于线上产品环境。 如果请求未指定执行哪个PHP文件，则默认执行目录内的index.php 或者 index.html。如果这两个文件都不存在，服务器会返回404错误。 当你在命令行启动这个Web Server时，如果指定了一个PHP文件，则这个文件会作为一个“路由”脚本，意味着每次请求都会先执行这个脚本。如果这个脚本返回 FALSE ，那么直接返回请求的文件（例如请求静态文件不作任何处理）。否则会把输出返回到浏览器。 php -S localhost:8000 服务于当前目录 php -S localhost:8000 -t foo/ 启动时指定根目录 php -S localhost:8000 router.php 使用路由（Router）脚本]]></content>
      <categories>
        <category>php</category>
      </categories>
      <tags>
        <tag>php linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[devops-api notes]]></title>
    <url>%2F2019%2F07%2F22%2Fdevops-api-notes%2F</url>
    <content type="text"><![CDATA[token 暂不需要 关了 magicpush init //生成token //配置token magicpush token –create=xiaoayong –root-token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJuYW1lIjoicm9vdCIsInVwZGF0ZVRpbWUiOjE1MzA5NDIzNzJ9.-NUb3vqQQYs_83TliNDlXqHcuMlEWP8FDBj2GdDZgyE nohup magicpush server --mode=dev &amp; ##linux 请求头 MAGICPUSH-TOKEN 加上生成的 eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJuYW1lIjoieGlhb2F5b25nIiwidXBkYXRlVGltZSI6MTUzMDk0MjQ0Nn0.KS1CrMPC5A4sKTMC1AcPoF-soSUmELa8FE5dgZAv1GY 一：http://127.0.0.1:7070/api/v1/sendmsg/dingding post 不要添加请求头 form-data发送一下内容 POST /api/v1/sendmsg/dingding msgType： text|markdown msg: 要发送的消息 title: 发送markdown消息时需要指定此参数，指定标题 url: 可以指定钉钉机器人的URL，这样就不用在conf/app.conf 配置钉钉机器人的URL 注意: 在发送markdown消息时，markdown的语法可以查看官方文档 二 http://127.0.0.1:7070/api/v1/sendmsg/weixin post form-data msgType text 消息类型,目前只支持文本消息 toTag 1 toUser XiaoAYong toParty 3 msg aa 我的脚本执行状况\n[脚本描述：测试]\n[脚本描述：测试]\n[脚本执行时间：20180629]\n 三 设置节假日和工作日 POST /api/v1/holiworkday 2018年 { “year”: “2018”, “holiday”: [ { “name”: “yuandan”, “zh_name”: “元旦”, “start_time”: “2018-01-01”, “end_time”: “2018-01-01” }, { “name”: “chunjie”, “zh_name”: “春节”, “start_time”: “2018-02-15”, “end_time”: “2018-02-21” }, { “name”: “qingming”, “zh_name”: “清明节”, “start_time”: “2018-04-05”, “end_time”: “2018-04-07” }, { “name”: “laodong”, “zh_name”: “劳动节”, “start_time”: “2018-04-29”, “end_time”: “2018-05-01” }, { “name”: “duanwu”, “zh_name”: “端午节”, “start_time”: “2018-06-16”, “end_time”: “2018-06-18” }, { “name”: “zhongqiu”, “zh_name”: “中秋节”, “start_time”: “2018-09-22”, “end_time”: “2018-09-24” }, { “name”: “guoqing”, “zh_name”: “国庆节”, “start_time”: “2018-10-01”, “end_time”: “2018-10-07” } ], “workday”: [ “2018-02-11”, “2018-02-24”, “2018-04-08”, “2018-04-28”, “2018-09-29”, “2018-09-30” ] } 2019年 { “year”: “2019”, “holiday”: [ { “name”: “yuandan”, “zh_name”: “元旦”, “start_time”: “2019-01-01”, “end_time”: “2019-01-01” }, { “name”: “chunjie”, “zh_name”: “春节”, “start_time”: “2019-02-04”, “end_time”: “2019-02-10” }, { “name”: “qingming”, “zh_name”: “清明节”, “start_time”: “2019-04-05”, “end_time”: “2019-04-07” }, { “name”: “laodong”, “zh_name”: “劳动节”, “start_time”: “2019-05-01”, “end_time”: “2019-05-04” }, { “name”: “duanwu”, “zh_name”: “端午节”, “start_time”: “2019-06-07”, “end_time”: “2019-06-09” }, { “name”: “zhongqiu”, “zh_name”: “中秋节”, “start_time”: “2019-09-13”, “end_time”: “2019-09-15” }, { “name”: “guoqing”, “zh_name”: “国庆节”, “start_time”: “2019-10-01”, “end_time”: “2019-10-07” } ], “workday”: [ “2018-02-02”, “2018-02-03”, “2018-04-28”, “2018-05-05”, “2018-09-29”, “2018-10-12” ] } 四 判断给定的日期是节假日工作日周末 GET /api/v1/holiworkday?date=2018-08-25 五 IP地址查询 GET /api/v1/queryip?ip=xxx.xxx.xxx.xxx http://127.0.0.1:7070/api/v1/queryip?ip=47.75.129.108 六 GET /api/v1/queryphone?phone=手机号 http://127.0.0.1:7070/api/v1/queryphone?phone=18612309765 七 windows操作指南 SET CGO_ENABLED=0 SET GOOS=windows go build -o devops-win-amd64.exe main.go start /b devops-win-amd64.exe server –mode=dev 八 方糖server 原始请求 https://sc.ftqq.com/$key.send?text=order_attachment脚本错误&amp;desp=正式服务器 http://127.0.0.1:7070/api/v1/weixin?text=order_attachment脚本错误&amp;desp=正式服务器]]></content>
  </entry>
  <entry>
    <title><![CDATA[talang work notes]]></title>
    <url>%2F2019%2F07%2F22%2Ftalang-work-notes%2F</url>
    <content type="text"><![CDATA[talang–notes sz DashboardController.php 保存服务器文件到本地 php -f encrypt.php &gt;test.txt 再redis-cli SADD time_user_whitelist 84 假如白名单 SISMEMBER time_user_whitelist 333 //判断uid333在不在白名单 SCARD time_user_whitelist //获取集合个数 SMEMBERS key //获取集合所有成员 Ctrl + H 输入^，然后Find All，查找所有的行首 输入$，然后Find All，查找所有的行尾 const NEW_ORDER = 10;//新订单 const AUDIT_LOADING = 40;//审批中 const AUDIT_SUCCESS = 60;//审批成功 const AUDIT_REFUSED = 50;//审批拒绝 const BLACK_REFUSED = 30;//欺诈拒绝 const WAIT_INVESTOR = 75;//待资金方确认 const LENDING_LOADING = 80;//放款处理中 const AGAIN_LENDING = 85;//待重新放款 const LENDING_SUCCESS = 90;//放款成功 const LENDING_FAILED = 100;//放款失败 const ROLLOVER_DEAL = 115;//展期处理中 const PREREPAY_LOADING = 130;//提前还款处理中 const NORMAL_LOADING = 110;//正常还款中 const LOAN_CANCEL = 70;//贷款取消 const PRE_SETTLE = 150;//提前结清 const NORMAL_SETTLE = 140;//正常结清 const OVERDUE_SETTLE = 160;//逾期结清 const OVERDUE = 120;//已逾期 ***core.repayment_plan表里status的值 有 1 2 3 4 5 6 这些状态********** select id,`status` FROM core.repayment_plan where 1=1 GROUP BY `status`; 状态：1 新建，2 成功，3 失败，4 由于提前还款账单取消 5 已申请提前还款 6 还款处理中 7 因提前还款失败而废弃 8 展期处理中 9 展期成功 10 因展期成功而废弃 ***core.repayment_plan表里overdue_status的值 有 0 1 这些状态********** select id,`overdue_status` FROM core.repayment_plan where 1=1 GROUP BY `overdue_status`; 还款计划是否有过逾期 0-未逾过期 1-逾过期 ***core.repayment_fees表里type的值 有 1 2 3 这些状态********** select id,`type` FROM core.repayment_fees where 1=1 GROUP BY `type`; 还款类型，1 一次性服务费,2 正常还款,3 提前还款 ***core.repayment_fees表里fee_type的值 有 1 2 3 4 这些状态********** select id,`fee_type` FROM core.repayment_fees where 1=1 GROUP BY `fee_type`; 还款的费用类型，1 本金,2 利息, 3 一次性服务费, 4 逾期利息 5//展期服务费 ***core.repayment_fees表里status的值 有 1 2 3 4 5 6 这些状态********** select id,`status` FROM core.repayment_fees where 1=1 GROUP BY `status`; 费用状态，已经结清，部分结清，未结清 ???????? 同 状态：1 新建，2 成功，3 失败，4 由于提前还款账单取消 5 已申请提前还款 6 还款处理中 7 因提前还款失败而废弃 8 展期处理中 9 展期成功 10 因展期成功而废弃]]></content>
  </entry>
  <entry>
    <title><![CDATA[hexo笔记]]></title>
    <url>%2F2019%2F07%2F22%2Fhexo%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[hexo安装及常用方式安装及常用命令 https://www.cnblogs.com/debugxw/p/11006734.html npm install -g hexo #获得原来的镜像地址 npm get registry #设置为淘宝镜像 npm config set registry http://registry.npm.taobao.org/ #换成原来的 npm config set registry https://registry.npmjs.org/ hexo new &quot;postName&quot; #新建文章 hexo new page &quot;pageName&quot; #新建页面 hexo generate #生成静态页面至public目录 hexo server #开启预览访问端口（默认端口4000，&apos;ctrl + c&apos;关闭server） hexo deploy #部署到GitHub hexo help # 查看帮助 hexo version #查看Hexo的版本 #缩写 hexo n == hexo new hexo g == hexo generate hexo s == hexo server hexo d == hexo deploy #组合命令 hexo s -g #生成并本地预览 hexo d -g #生成并上传 hexo g ## 生成 hexo s ## 启动服务 打开浏览器访问 http://localhost:4000 即可看到内容 首先，ssh key肯定要配置好。 其次，配置_config.yml中有关deploy的部分： deploy: type: git repository: git@github.com:xay216216/xay216216.github.io.git branch: master &lt;center&gt;hexo s -g #生成并本地预览&lt;/center&gt; hexo d -g #生成并上传 日常hexo操作 在E:\phpStudy\PHPTutorial\WWW\hexo 打开git bash hexo n ‘hexo笔记’ 然后 E:\phpStudy\PHPTutorial\WWW\hexo\source_posts\hexo笔记.md 修改文件 然后保存 hexo s ##启动服务 关掉服务 hexo d -g #生成并上传 友情链接 https://www.jianshu.com/p/3a05351a37dc]]></content>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sudo的用法]]></title>
    <url>%2F2019%2F03%2F15%2Fsudo%E7%9A%84%E7%94%A8%E6%B3%95%2F</url>
    <content type="text"><![CDATA[深度linux和ubuntu apt-cache search # ——(package 搜索包) apt-cache show #——(package 获取包的相关信息，如说明、大小、版本等) sudo apt-get install # ——(package 安装包)sudo apt-get install # —–(package - - reinstall 重新安装包) sudo apt-get -f install # —–(强制安装?#”-f = –fix-missing”当是修复安装吧…) sudo apt-get remove #—–(package 删除包) sudo apt-get remove - - purge # ——(package 删除包，包括删除配置文件等) sudo apt-get autoremove –purge # —-(package 删除包及其依赖的软件包+配置文件等（只对6.10有效，强烈推荐）) sudo apt-get update #——更新源 sudo apt-get upgrade #——更新已安装的包 sudo apt-get dist-upgrade # ———升级系统 sudo apt-get dselect-upgrade #——使用 dselect 升级 apt-cache depends #——-(package 了解使用依赖) apt-cache rdepends # ——(package 了解某个具体的依赖?#当是查看该包被哪些包依赖吧…) sudo apt-get build-dep # ——(package 安装相关的编译环境) apt-get source #——(package 下载该包的源代码) sudo apt-get clean &amp;&amp; sudo apt-get autoclean # ——–清理下载文件的存档 &amp;&amp; 只清理过时的包 sudo apt-get check #——-检查是否有损坏的依赖 apt-get install apt-get remove的用法 apt-get remove [–purge] apt-get update apt-cache search的用法 apt-cache search apt-cache show 的用法 apt-cache show apt-cache showpkg 的用法 apt-cache showpkg sudo systemctl start php-fpm sudo systemctl start nginx sudo systemctl enable php-fpm sudo systemctl enable nginx 重启SSH服务，命令：systemctl restart sshd.service centos 7 linux命令 systemctl start mysqld.service 启动mysql服务 systemctl status mysqld.service 查看服务状态 mysql -uroot -p 链接mysql yum list installed | grep php 列出phpan安装的信息 yum remove php70w.x86_64 下载 rpm -qa | grep mysql 查看MySQL安装信息 centos安装PHP7.0 PHP-fpm nginx http://os.51cto.com/art/201703/534277.htm yum -y install epel-release yum -y install nginx rpm -Uvh https://mirror.webtatic.com/yum/el7/webtatic-release.rpm yum -y install php70w-fpm php70w-cli php70w-gd php70w-mcrypt php70w-mysql php70w-pear php70w-xml php70w-mbstring php70w-pdo php70w-json php70w-pecl-apcu php70w-pecl-apcu-devel]]></content>
      <tags>
        <tag>sudo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git使用命令]]></title>
    <url>%2F2019%2F03%2F15%2Fgit%E4%BD%BF%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[常用git命令 git clone ** 建仓库 cd 到仓库 git status 查看当前状态 git pull origin master （加入本地现在是master分支） git checkout -b KF-660 基于master分支建一个本地KF-660 git设置alias [alias] st = status co = checkout ci = commit br = branch rt = remote last = log -1 –name-status al = log –pretty=format:’%h %cn -%d %s (%cd)’ –abbrev-commit -30 l = log –graph –pretty=format:’%Cred%h%Creset %Cgreen%cn -%C(cyan)%d%Creset %s %Cgreen(%cd)%Creset’ –abbrev-commit -30 ll = log –name-status –graph –pretty=format:’%Cred%h%Creset %Cgreen%cn -%C(cyan)%d%Creset %s %Cgreen(%cd)%Creset’ –abbrev-commit -10 现在让我们来重置回第一次提交的状态： git status git checkout 文件名 取消修改某文件 git log git reset –hard b7057a9 本地回滚到上次的某个版本号 git status 回滚后,再次查看当前本地分支状态 git add . git commit -m ‘增加’ 提交 git pull origin KF-660 拉去远端最新的KF-660 git push origin KF-660 推到远端KF-660分支 git checkout develop git pull origin develop 拉去最新的develop分支 git merge KF-660 develop分支合KF-660的代码 git push origin develop 推到远端的develop分支推到正式 git checkout master git pull origin master git merge KF-660 git push origin master git tag v1.1.5.35 打一个tag号 git push origin v1.1.5.35 查找问题–查看文件改动 git blame -L 183,+100 app/controllers/web/NewsController.php 查看某个文件第183行的改动记录，取100行记录 git log –pretty=oneline app/controllers/web/NewsController.php 文件的所有的改动历史 查看、添加、提交、删除、找回，重置修改文件 git help # 显示command的help git show # 显示某次提交的内容 git show $id git co – # 抛弃工作区修改 git co . # 抛弃工作区修改 git add # 将工作文件修改提交到本地暂存区 git add . # 将所有修改过的工作文件提交暂存区 git rm # 从版本库中删除文件 git rm –cached # 从版本库中删除文件，但不删除文件 git reset # 从暂存区恢复到工作文件 git reset – . # 从暂存区恢复到工作文件 git reset –hard # 恢复最近一次提交过的状态，即放弃上次提交后的所有本次修改 git ci git ci . git ci -a # 将git add, git rm和git ci等操作都合并在一起做 git ci -am “some comments” git ci –amend # 修改最后一次提交记录 git revert &lt;$id&gt; # 恢复某次提交的状态，恢复动作本身也创建次提交对象 git revert HEAD # 恢复最后一次提交的状态 git commit之后，想撤销commit git reset --soft HEAD^ HEAD^的意思是上一个版本，也可以写成HEAD~1 如果你进行了2次commit，想都撤回，可以使用HEAD~2 --mixed 意思是：不删除工作空间改动代码，撤销commit，并且撤销git add . 操作 这个为默认参数,git reset --mixed HEAD^ 和 git reset HEAD^ 效果是一样的。 --soft 不删除工作空间改动代码，撤销commit，不撤销git add . --hard 删除工作空间改动代码，撤销commit，撤销git add . 注意完成这个操作后，就恢复到了上一次的commit状态。 顺便说一下，如果commit注释写错了，只是想改一下注释，只需要： git commit --amend 此时会进入默认vim编辑器，修改注释完毕后保存就好了。 查看我的分支和 master 的不同 git diff master..my-branch 编辑上次提交git commit --amend -m &quot;更好的提交日志&quot; 查看文件diff git diff # 比较当前文件和暂存区文件差异 git diff git diff # 比较两次提交之间的差异 git diff .. # 在两个分支之间比较 git diff –staged # 比较暂存区和版本库差异 git diff –cached # 比较暂存区和版本库差异 git diff –stat # 仅仅比较统计信息 查看提交记录 git log git log # 查看该文件每次提交记录 git log -p # 查看每次详细修改内容的diff git log -p -2 # 查看最近两次详细修改内容的diff git log –stat #查看提交统计信息 一般配置 git –version //查看git的版本信息 git config –global user.name //获取当前登录的用户 git config –global user.email //获取当前登录用户的邮箱 git config –global user.name ‘userName’ //设置git账户，userName为你的git账号， git config –global user.email ‘email’ 友情链接 https://sinchie.com/posts/git-remark/比较的是历史区和工作区的差异（修改） git diff master撤回内容(如果修改了工作区的文件后发现改错了，可以用暂存区或者版本库里的文件替换掉工作区的文件)用暂存区中的内容或者版本库中的内容覆盖掉工作区 git checkout index.html取消增加到暂存区的内容（添加时） git reset HEAD index.html//显示目录的状体 有没有添加或者修改文件 git status]]></content>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[welcome to kobe's blog]]></title>
    <url>%2F2018%2F09%2F06%2Fwelcome-to-my-blog%2F</url>
    <content type="text"><![CDATA[阿飘莱勇飞趣–认真创造，安静面对，乐观变化，幸福时间，美。]]></content>
  </entry>
</search>
